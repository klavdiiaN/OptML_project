{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e359ffb-71a2-43db-87a3-4de42e63aa55",
      "metadata": {
        "id": "9e359ffb-71a2-43db-87a3-4de42e63aa55"
      },
      "source": [
        "# 1.Imports & environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Dmx7bbzJS6S4",
      "metadata": {
        "id": "Dmx7bbzJS6S4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NosAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, gamma=0, lr_decay=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= gamma:\n",
        "            raise ValueError(\"Invalid gamma value: {}\".format(gamma))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, gamma=gamma, lr_decay=lr_decay)\n",
        "        super(NosAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(NosAdam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('lr_decay', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                # amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                    state['B_old'] = 0\n",
        "                    state['B_new'] = 1\n",
        "                    # if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        # state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                # if amsgrad:\n",
        "                #     max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "                beta2 = state['B_old']/state['B_new']\n",
        "                gamma = group['gamma']\n",
        "                # pnorm = group['pnorm']\n",
        "                lr_decay = group['lr_decay']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                step = state['step']\n",
        "                state['B_old'] += math.pow(step, -gamma)\n",
        "                state['B_new'] += math.pow(step+1, -gamma)\n",
        "\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # if pnorm == 2:\n",
        "                #     exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                # elif pnorm ==3:\n",
        "                #     # exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, np.multiply(grad, grad))\n",
        "                #     exp_avg_sq = beta2*exp_avg_sq + (1 - beta2) * np.power(grad, 3)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                # if amsgrad:\n",
        "                #     # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                #     torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                #     # Use the max. for normalizing running avg. of gradient\n",
        "                #     denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                # else:\n",
        "                # if pnorm ==2:\n",
        "                #     denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                # elif pnorm==3: # pnorm = 3\n",
        "                #     denom = np.cbrt(exp_avg_sq) + group['eps']\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # AdaStab no longer needs bias correction for v_t\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "                step_size = group['lr'] / bias_correction1\n",
        "                if lr_decay:\n",
        "                    step_size = step_size/math.sqrt(state['step'])\n",
        "\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def denominator(self):\n",
        "        denom = np.array([0])\n",
        "        denom_sum = 0\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "                exp_avg_sq = state[\"exp_avg_sq\"]\n",
        "\n",
        "                exp_avg_sq = exp_avg_sq.numpy()\n",
        "                exp_avg_sq = np.reshape(exp_avg_sq, -1)\n",
        "                denom = np.concatenate((denom, exp_avg_sq))\n",
        "                denom_sum += torch.sum(exp_avg_sq)\n",
        "        return denom, denom_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a167aec-3ddd-4067-8b5f-77af4fe2854f",
      "metadata": {
        "id": "2a167aec-3ddd-4067-8b5f-77af4fe2854f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "import sys\n",
        "# from NosAdam import NosAdam\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" #magic line to deal with my versions of libraries\n",
        "\n",
        "# Setup predictable randomization\n",
        "seed = 10\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Setup CUda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c42f77e-92d5-4cb3-9099-b699d7c7858d",
      "metadata": {
        "id": "8c42f77e-92d5-4cb3-9099-b699d7c7858d"
      },
      "source": [
        "# 2. Loading and preparation of data\n",
        "As a basis for comparison we will be using the MNIST dataset. If we manage to do all the work we want, we will then use other datasets for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3668867e-acbe-4676-84dc-1911b88c0729",
      "metadata": {
        "id": "3668867e-acbe-4676-84dc-1911b88c0729"
      },
      "source": [
        "### 2.1. Definition of methods to extract data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "febd210c-2240-45d1-8860-14be25f58c40",
      "metadata": {
        "id": "febd210c-2240-45d1-8860-14be25f58c40"
      },
      "outputs": [],
      "source": [
        "def extract_data(filename, image_shape, image_number):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        bytestream.read(16)\n",
        "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
        "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_labels(filename, image_number):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        bytestream.read(8)\n",
        "        buf = bytestream.read(1 * image_number)\n",
        "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a048581f-bdb6-4a22-9b0c-9f371c1303b3",
      "metadata": {
        "id": "a048581f-bdb6-4a22-9b0c-9f371c1303b3"
      },
      "source": [
        "### 2.2. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c7c71149-8c1e-4fbd-b2b9-2e332fcf8e1b",
      "metadata": {
        "id": "c7c71149-8c1e-4fbd-b2b9-2e332fcf8e1b"
      },
      "outputs": [],
      "source": [
        "image_shape = (28, 28)\n",
        "train_set_size = 60000\n",
        "test_set_size = 10000\n",
        "data_folder = 'mnist_data'\n",
        "\n",
        "train_images_path = os.path.join(data_folder, 'train-images-idx3-ubyte.gz')\n",
        "train_labels_path = os.path.join(data_folder, 'train-labels-idx1-ubyte.gz')\n",
        "test_images_path = os.path.join(data_folder, 't10k-images-idx3-ubyte.gz')\n",
        "test_labels_path = os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
        "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
        "train_labels = extract_labels(train_labels_path, train_set_size)\n",
        "test_labels = extract_labels(test_labels_path, test_set_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff8b6f8-0472-4936-a88d-cde07e2359b4",
      "metadata": {
        "id": "eff8b6f8-0472-4936-a88d-cde07e2359b4"
      },
      "source": [
        "### 2.3. Convert data from numpy arrays to torch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b3b5bfe7-685e-4adc-8a48-fc1a25a39381",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3b5bfe7-685e-4adc-8a48-fc1a25a39381",
        "outputId": "2bec7a28-6486-4b20-c36f-3b08dcaee38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training features: torch.Size([60000, 28, 28]) \n",
            "Testing features: torch.Size([10000, 28, 28])\n",
            "Training labels: torch.Size([60000]) \n",
            "Testing labels: torch.Size([10000])\n"
          ]
        }
      ],
      "source": [
        "features_train=torch.from_numpy(train_images).to(device)\n",
        "features_test=torch.from_numpy(test_images).to(device)\n",
        "print('Training features:', features_train.shape, '\\n'\n",
        "'Testing features:', features_test.shape)\n",
        "\n",
        "labels_train=torch.from_numpy(train_labels).to(device)\n",
        "labels_test=torch.from_numpy(test_labels).to(device)\n",
        "print('Training labels:', labels_train.shape, '\\n'\n",
        "'Testing labels:', labels_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072269a3-e3f6-470c-8ca9-58a0555d9851",
      "metadata": {
        "id": "072269a3-e3f6-470c-8ca9-58a0555d9851"
      },
      "source": [
        "### 2.4. Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d24f2cbd-0279-44d6-89d2-1dca355248df",
      "metadata": {
        "id": "d24f2cbd-0279-44d6-89d2-1dca355248df"
      },
      "outputs": [],
      "source": [
        "mean, std = features_train.float().mean(), features_train.float().std()\n",
        "\n",
        "features_train = features_train.float().sub_(mean).div_(std)\n",
        "features_test = features_test.float().sub_(mean).div_(std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "gO2vsS-k6GY9",
      "metadata": {
        "id": "gO2vsS-k6GY9"
      },
      "outputs": [],
      "source": [
        "#reshape to make the 1st channel be batch size\n",
        "\n",
        "features_train = features_train.reshape(-1, 1, 28, 28)\n",
        "features_test = features_test.reshape(-1, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc94097-21c0-4308-ba10-af4dc6d04f30",
      "metadata": {
        "id": "bdc94097-21c0-4308-ba10-af4dc6d04f30"
      },
      "source": [
        "# 3. Setting up networks and evaluation methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93eeb387-7739-466e-972b-3f10a9a358a7",
      "metadata": {
        "id": "93eeb387-7739-466e-972b-3f10a9a358a7"
      },
      "source": [
        "### 3.1. Multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13013745-d51c-4348-9f26-211b04d3cc1d",
      "metadata": {
        "id": "13013745-d51c-4348-9f26-211b04d3cc1d"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_size_1=512, hidden_size_2=100, hidden_size_3=10):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, hidden_size_1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_size_1, hidden_size_2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_size_2, hidden_size_3))\n",
        "    \n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "mlp = MLP()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g3hZg487lszk",
      "metadata": {
        "id": "g3hZg487lszk"
      },
      "source": [
        "###3.2. Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "yUu42pH8l5md",
      "metadata": {
        "id": "yUu42pH8l5md"
      },
      "outputs": [],
      "source": [
        "from numpy.ma.core import reshape\n",
        "class CNN(nn.Module):\n",
        "   def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=5, stride=1, padding='same')\n",
        "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=5, stride=1, padding='same')\n",
        "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "   def forward(self, input:torch.Tensor) -> torch.Tensor:\n",
        "        pool1 = torch.max_pool2d(F.relu(self.conv1(input)), kernel_size=2, stride=2)\n",
        "        pool2 = torch.max_pool2d(F.relu(self.conv2(pool1)), kernel_size=2, stride=2)\n",
        "        res = pool2.reshape(-1, 64*7*7)\n",
        "        hidden = F.relu(self.fc1(res))\n",
        "        output = self.fc2(hidden)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e119d2d9-562a-4bdf-8ce7-eea9d59272fa",
      "metadata": {
        "id": "e119d2d9-562a-4bdf-8ce7-eea9d59272fa"
      },
      "source": [
        "### 3.3. Implementation of method for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8e5ca3dd-313e-4c15-99da-10937d42595c",
      "metadata": {
        "id": "8e5ca3dd-313e-4c15-99da-10937d42595c"
      },
      "outputs": [],
      "source": [
        "def run_nn(x_train, y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch):\n",
        "    # loss_all_train, loss_all_test = [], []\n",
        "    loss_train_ret = 0\n",
        "    loss_test_ret = 0\n",
        "    loss_train = 0\n",
        "    # epochs_all = torch.arange(1, num_epoch+num_epoch/10, num_epoch/10)\n",
        "    # epochs_all[-1] = num_epoch - 1\n",
        "            \n",
        "    for epoch in range(num_epoch):\n",
        "        for b in range(0, x_train.size(0), size_minibatch):\n",
        "            # y = model(x_train.narrow(0, b, size_minibatch))\n",
        "            # loss_train = criterion(y, y_train.narrow(0, b, size_minibatch))\n",
        "            y = model(x_train[b:b+size_minibatch])\n",
        "            loss_train = criterion(y, y_train[b:b+size_minibatch])\n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch == num_epoch - 1:\n",
        "            # loss_train = loss_train.detach().numpy()\n",
        "            # loss_all_train.append(loss_train)\n",
        "\n",
        "            y_test_obt = model(x_test)\n",
        "            loss_test = criterion(y_test_obt, y_test)\n",
        "            # loss_test = loss_test.detach().numpy()\n",
        "            # loss_all_test.append(loss_test)\n",
        "            \n",
        "            loss_train_ret = loss_train\n",
        "            loss_test_ret = loss_test\n",
        "            \n",
        "            print('Final, Train Loss: %.4f, Test Loss: %.4f' %(loss_train, loss_test))\n",
        "\n",
        "    return loss_train_ret, loss_test_ret"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93565277-6e32-47ca-8c17-55098b57d7bb",
      "metadata": {
        "id": "93565277-6e32-47ca-8c17-55098b57d7bb"
      },
      "source": [
        "# 4. Metrics of our tuning protocol\n",
        "At this stage, we want to select the hyperparameter search space for each optimizer. This way, we can first tune the hyperparameters of each optimizer separately and then select the trial that achieved lowest final validation error.\n",
        "We then comapre the optimizers' performance by looking at the validation and test errors as suggested in the paper \"On empirical comparisons of optimizers for deep learning\".\n",
        "\n",
        "We will also look at the training speed (number of training steps required) to reach a traget validation error.\n",
        "\n",
        "Everything is tuned on a log scale.\n",
        "\n",
        "No L_2 regularization or weight decay is used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "124b4d1e-5dbb-4a8a-a365-eb2ce1ecafc7",
      "metadata": {
        "id": "124b4d1e-5dbb-4a8a-a365-eb2ce1ecafc7"
      },
      "source": [
        "### 4.1. Tuning protocol using bootstrap\n",
        "To estimate means and uncertainties of our tuning protocol we will use bootstrapping starting from an initial search space suggested by the paper \"On Empirical Comparisons of Optimizers for Deep Learning\".\n",
        "We run N trials by randomly picking values in the search space of the algorithm at every trial.\n",
        "Then we sample these trials with replacement and compute our statistic on the first K trials of this sample. We repeat this process 100 times and compute the 5th percentile and 95th percentile of the bootstrap distribution.\n",
        "\n",
        "This allows us to plot the error bars to show the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a304d9-db74-4bc5-a12c-b97b971097bd",
      "metadata": {
        "id": "92a304d9-db74-4bc5-a12c-b97b971097bd"
      },
      "source": [
        "### 4.2. Tuning NosAdam for the MLP on MNIST\n",
        "The hyperparameters we are tuning are alpha_0/epsilon, 1 - beta_1, 1 - beta_2, epsilon.\n",
        "The initial search spaces are suggested based on the experience of the writers of the same paper, \"On empirical comparisons of optimizers for deep learning\".\n",
        "N is also suggested to be 500 and K to be 100."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "465c3fd8-c425-4ce5-991a-0ad3cd2ba426",
      "metadata": {
        "id": "465c3fd8-c425-4ce5-991a-0ad3cd2ba426"
      },
      "source": [
        "##### Set up model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d84470-824e-4386-a781-c9df9bb78f10",
      "metadata": {
        "id": "52d84470-824e-4386-a781-c9df9bb78f10"
      },
      "outputs": [],
      "source": [
        "# Model fixed parameters\n",
        "model = MLP()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device) # good loss function for classification tasks\n",
        "num_epoch = 50\n",
        "size_minibatch = 128\n",
        "\n",
        "x_train = features_train\n",
        "y_train = labels_train\n",
        "x_test = features_test\n",
        "y_test = labels_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ca30bc-c245-45ab-8d7e-ec9c37d62ab3",
      "metadata": {
        "id": "25ca30bc-c245-45ab-8d7e-ec9c37d62ab3",
        "tags": []
      },
      "source": [
        "##### Tune to find best parameter\n",
        "We perform trials until we have K of them, then we pick the best based on our statistic of interest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mGYqlhu0eJwA",
      "metadata": {
        "id": "mGYqlhu0eJwA"
      },
      "source": [
        "#### 4.2.1. Initial search for best hyperparameters for **NosAdam** optimizer on **MLP**. K= 100. \n",
        "Interrupted because of nan loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0pqeK5opwRvk",
      "metadata": {
        "id": "0pqeK5opwRvk"
      },
      "source": [
        "##### Set up parameters and search space for the initial trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FhGg_EqiwWKJ",
      "metadata": {
        "id": "FhGg_EqiwWKJ"
      },
      "outputs": [],
      "source": [
        "N = 200\n",
        "K = 100 # Number of trials being kept for the statistic\n",
        "\n",
        "# Initial search spaces for parameters\n",
        "alpha_0 = np.linspace(10**(-2), 10**(4), N)\n",
        "beta_1 = np.linspace(10**(-3), 1, N)\n",
        "beta_2 = np.linspace(10**(-4), 1, N)\n",
        "eps = np.linspace(10**(-10), 10**(10), N)\n",
        "\n",
        "# TODO: tune number of decay steps between 0.5 and 1 times the number of training steps\n",
        "# TODO : tune learning rate decay factor within 10**-3, 10**-2, 10**-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W09RkUufwxAx",
      "metadata": {
        "id": "W09RkUufwxAx"
      },
      "source": [
        "Perform search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c167327-ee1a-42e9-b628-14f61022aef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5c167327-ee1a-42e9-b628-14f61022aef4",
        "outputId": "6d2265bf-ca19-4229-b80d-212451de5ecd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Claudia Naumova\\Opt_ML\\OptML_project\\NosAdam.py:86: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: 0.5216, Test Loss: 0.4796\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\CLAUDI~1\\AppData\\Local\\Temp/ipykernel_8072/1663774356.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtrain_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_minibatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Concatenate hyperparameters with results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Users\\CLAUDI~1\\AppData\\Local\\Temp/ipykernel_8072/550642988.py\u001b[0m in \u001b[0;36mrun_nn\u001b[1;34m(x_train, y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m# y = model(x_train.narrow(0, b, size_minibatch))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;31m# loss_train = criterion(y, y_train.narrow(0, b, size_minibatch))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msize_minibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msize_minibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Claudia Naumova\\miniconda3\\envs\\knvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Users\\CLAUDI~1\\AppData\\Local\\Temp/ipykernel_8072/3081095880.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Claudia Naumova\\miniconda3\\envs\\knvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Claudia Naumova\\miniconda3\\envs\\knvenv\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Claudia Naumova\\miniconda3\\envs\\knvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Claudia Naumova\\miniconda3\\envs\\knvenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Claudia Naumova\\miniconda3\\envs\\knvenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "nb_hyperamaters_to_tune = 4\n",
        "nb_exported_statistics  = 2\n",
        "\n",
        "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
        "\n",
        "for _ in range(K):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model=MLP()\n",
        "    model=model.to(device)\n",
        "    optimizer = NosAdam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Concatenate hyperparameters with results\n",
        "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
        "    \n",
        "    # Check wether we have the smallest test error and store parameters in case we find it\n",
        "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
        "        lowest_test_error = vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6yOLXHgin-Ve",
      "metadata": {
        "id": "6yOLXHgin-Ve"
      },
      "source": [
        "#### 4.2.2. Final search for best hyperparameters for **NosAdam** optimizer on **MLP**. K = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WYEeVzSvw-fz",
      "metadata": {
        "id": "WYEeVzSvw-fz"
      },
      "source": [
        "##### Set up parameters and search space for the final trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4jUNhWkVv96z",
      "metadata": {
        "id": "4jUNhWkVv96z"
      },
      "outputs": [],
      "source": [
        "N = 200\n",
        "K = 50 # Number of trials being kept for the statistic\n",
        "\n",
        "# Final search spaces for parameters\n",
        "alpha_0 = np.linspace(10**(-1), 10, N)\n",
        "beta_1 = np.linspace(10**(-3), 1, N)\n",
        "beta_2 = np.linspace(10**(-4), 1, N)\n",
        "eps = np.linspace(10**(-6), 10**(-2), N)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4gXEoY3nxDaO",
      "metadata": {
        "id": "4gXEoY3nxDaO"
      },
      "source": [
        "Perform search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RA5uFjwh2-V8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA5uFjwh2-V8",
        "outputId": "b4d877f0-78da-4fbe-bb34-91a380f2921d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Claudia Naumova\\Opt_ML\\OptML_project\\NosAdam.py:86: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final, Train Loss: 0.0001, Test Loss: 0.0810\n",
            "Final, Train Loss: 1.8264, Test Loss: 1.8137\n",
            "Final, Train Loss: 0.2071, Test Loss: 0.1658\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0994\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0387, Test Loss: 0.2560\n",
            "Final, Train Loss: 0.2790, Test Loss: 0.4581\n",
            "Final, Train Loss: 1.7682, Test Loss: 1.7026\n",
            "Final, Train Loss: 2.3060, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0476, Test Loss: 0.2908\n",
            "Final, Train Loss: 0.1852, Test Loss: 0.3003\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1531\n",
            "Final, Train Loss: 2.3071, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.0236, Test Loss: 0.3225\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1110\n",
            "Final, Train Loss: 2.3067, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1217\n",
            "Final, Train Loss: 2.3075, Test Loss: 2.3027\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0028, Test Loss: 0.2247\n",
            "Final, Train Loss: 1.8098, Test Loss: 1.8529\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0846\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0049, Test Loss: 0.2980\n",
            "Final, Train Loss: 1.8408, Test Loss: 1.8101\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0822\n",
            "Final, Train Loss: 0.0015, Test Loss: 0.1492\n",
            "Final, Train Loss: 2.3054, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3059, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0798\n",
            "Final, Train Loss: 2.3067, Test Loss: 2.3018\n",
            "Final, Train Loss: 2.3063, Test Loss: 2.3017\n",
            "Final, Train Loss: 1.9559, Test Loss: 1.9828\n",
            "Final, Train Loss: 2.3054, Test Loss: 2.3018\n",
            "Final, Train Loss: 2.3055, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.3049\n",
            "Final, Train Loss: 0.1586, Test Loss: 0.3041\n",
            "Final, Train Loss: 2.3076, Test Loss: 2.3019\n",
            "Final, Train Loss: 2.3063, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3064, Test Loss: 2.3019\n",
            "Final, Train Loss: 2.3080, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.0874, Test Loss: 0.4211\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0885\n",
            "Final, Train Loss: 0.0064, Test Loss: 0.0652\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0935\n",
            "Final, Train Loss: 1.7625, Test Loss: 1.8797\n",
            "Final, Train Loss: 0.0431, Test Loss: 0.3156\n",
            "Final, Train Loss: 2.3065, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0304, Test Loss: 0.2404\n"
          ]
        }
      ],
      "source": [
        "nb_hyperamaters_to_tune = 4\n",
        "nb_exported_statistics  = 2\n",
        "\n",
        "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
        "\n",
        "\n",
        "for _ in range(K):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model = MLP()\n",
        "    model = model.to(device)\n",
        "    optimizer = NosAdam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Concatenate hyperparameters with results\n",
        "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
        "    \n",
        "    # Check wether we have the smallest test error and store parameters in case we find it\n",
        "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
        "        lowest_test_error = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f69a983b-b65e-4185-b56c-b6fabd784c4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f69a983b-b65e-4185-b56c-b6fabd784c4e",
        "outputId": "9b3ae363-9684-4ebd-d4e2-df7866b987d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beta 1: 0.25\n",
            "Beta 2: 0.84\n",
            "Epsilon: 1.52e-04\n",
            "Learning rate: 0.000068\n",
            "Train error: 0.006410\n",
            "Test error: 0.0652\n"
          ]
        }
      ],
      "source": [
        "# Print best parameters\n",
        "\n",
        "print('Beta 1: %.2f' % lowest_test_error[0])\n",
        "print('Beta 2: %.2f' % lowest_test_error[1])\n",
        "print('Epsilon: %.2e' % lowest_test_error[2])\n",
        "print('Learning rate: %.6f' % lowest_test_error[3])\n",
        "print('Train error: %.6f' % lowest_test_error[4])\n",
        "print('Test error: %.4f' % lowest_test_error[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5b6adf-dac0-4e2e-a3f4-27186b406140",
      "metadata": {
        "id": "3c5b6adf-dac0-4e2e-a3f4-27186b406140",
        "tags": []
      },
      "source": [
        "##### 4.2.3. Estimating trial outcomes via bootstrap\n",
        "At this stage we want to estimate means and uncertainties of our tuning protocol"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db030739-278f-4fe8-9932-b4af662923ff",
      "metadata": {
        "id": "db030739-278f-4fe8-9932-b4af662923ff"
      },
      "source": [
        "###### Run N trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad08fad-3e95-4d86-8d4d-38c2f43d6aa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ad08fad-3e95-4d86-8d4d-38c2f43d6aa3",
        "outputId": "f367bfbf-4529-4117-8b3d-e0e71a4cb2e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final, Train Loss: 1.8994, Test Loss: 1.7757\n",
            "Final, Train Loss: 0.2170, Test Loss: 0.1633\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0932\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0212, Test Loss: 0.2788\n",
            "Final, Train Loss: 1.6695, Test Loss: 1.4105\n",
            "Final, Train Loss: 0.9822, Test Loss: 0.9878\n",
            "Final, Train Loss: 1.6502, Test Loss: 1.6149\n",
            "Final, Train Loss: 0.5107, Test Loss: 0.3931\n",
            "Final, Train Loss: 0.0437, Test Loss: 0.2538\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1491\n",
            "Final, Train Loss: 2.0552, Test Loss: 1.9812\n",
            "Final, Train Loss: 0.0766, Test Loss: 0.2807\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1039\n",
            "Final, Train Loss: 2.2462, Test Loss: 2.0222\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1657\n",
            "Final, Train Loss: 2.3075, Test Loss: 2.3027\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0501, Test Loss: 0.3256\n",
            "Final, Train Loss: 2.3072, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0926\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.3562, Test Loss: 0.3096\n",
            "Final, Train Loss: 2.3078, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0832\n",
            "Final, Train Loss: 0.0009, Test Loss: 0.1810\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0959\n",
            "Final, Train Loss: 2.3067, Test Loss: 2.3018\n",
            "Final, Train Loss: 2.3063, Test Loss: 2.3017\n",
            "Final, Train Loss: 2.1297, Test Loss: 2.1450\n",
            "Final, Train Loss: 1.6065, Test Loss: 1.8496\n",
            "Final, Train Loss: 2.3054, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0748, Test Loss: 0.2777\n",
            "Final, Train Loss: 0.0012, Test Loss: 0.1972\n",
            "Final, Train Loss: 2.3077, Test Loss: 2.3029\n",
            "Final, Train Loss: 2.3061, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3015\n",
            "Final, Train Loss: 1.8642, Test Loss: 1.8050\n",
            "Final, Train Loss: 1.7734, Test Loss: 1.8047\n",
            "Final, Train Loss: 0.0379, Test Loss: 0.4267\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0960\n",
            "Final, Train Loss: 0.0125, Test Loss: 0.0665\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0959\n",
            "Final, Train Loss: 2.3063, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0322, Test Loss: 0.2729\n",
            "Final, Train Loss: 1.5329, Test Loss: 1.5964\n",
            "Final, Train Loss: 0.0074, Test Loss: 0.2242\n",
            "Final, Train Loss: 2.3079, Test Loss: 2.3022\n",
            "Final, Train Loss: 1.5331, Test Loss: 1.6054\n",
            "Final, Train Loss: 2.3051, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.2774, Test Loss: 0.3024\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1542\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3014\n",
            "Final, Train Loss: 0.0895, Test Loss: 0.0622\n",
            "Final, Train Loss: 0.1644, Test Loss: 0.3080\n",
            "Final, Train Loss: 0.0339, Test Loss: 0.3468\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3002\n",
            "Final, Train Loss: 1.7041, Test Loss: 1.9822\n",
            "Final, Train Loss: 2.3059, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1035\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0821\n",
            "Final, Train Loss: 2.3082, Test Loss: 2.3020\n",
            "Final, Train Loss: 0.0027, Test Loss: 0.1820\n",
            "Final, Train Loss: 2.3068, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3062, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.2808\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0786\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1290\n",
            "Final, Train Loss: 0.0010, Test Loss: 0.0707\n",
            "Final, Train Loss: 0.0293, Test Loss: 0.2893\n",
            "Final, Train Loss: 2.3071, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.0912, Test Loss: 0.2614\n",
            "Final, Train Loss: 0.1789, Test Loss: 0.1408\n",
            "Final, Train Loss: 2.3059, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0004, Test Loss: 0.0755\n",
            "Final, Train Loss: 1.1764, Test Loss: 1.5025\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.2214\n",
            "Final, Train Loss: 2.3060, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0316, Test Loss: 0.2884\n",
            "Final, Train Loss: 2.3054, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3068, Test Loss: 2.3021\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3070, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0328, Test Loss: 0.3055\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1852\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1030\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3062, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0024, Test Loss: 0.3195\n",
            "Final, Train Loss: 1.8168, Test Loss: 1.8257\n",
            "Final, Train Loss: 2.3051, Test Loss: 2.3023\n",
            "Final, Train Loss: 1.6224, Test Loss: 1.6795\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0855\n",
            "Final, Train Loss: 1.7588, Test Loss: 1.8434\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1068\n",
            "Final, Train Loss: 2.3055, Test Loss: 2.3014\n",
            "Final, Train Loss: 0.1704, Test Loss: 1.2043\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0941\n",
            "Final, Train Loss: 1.8687, Test Loss: 1.8567\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0908\n",
            "Final, Train Loss: 2.3061, Test Loss: 2.3024\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0915\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1848\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.2999\n",
            "Final, Train Loss: 0.0908, Test Loss: 0.3619\n",
            "Final, Train Loss: 0.2629, Test Loss: 0.4054\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0920\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0885\n",
            "Final, Train Loss: 2.1831, Test Loss: 2.0336\n",
            "Final, Train Loss: 0.0299, Test Loss: 0.3346\n",
            "Final, Train Loss: 0.0514, Test Loss: 0.3039\n",
            "Final, Train Loss: 2.3063, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3067, Test Loss: 2.3017\n",
            "Final, Train Loss: 1.7574, Test Loss: 1.8041\n",
            "Final, Train Loss: 2.3054, Test Loss: 2.3017\n",
            "Final, Train Loss: 1.8430, Test Loss: 1.8128\n",
            "Final, Train Loss: 1.5389, Test Loss: 1.7613\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3060, Test Loss: 2.2996\n",
            "Final, Train Loss: 1.8484, Test Loss: 1.8932\n",
            "Final, Train Loss: 1.3670, Test Loss: 2.2698\n",
            "Final, Train Loss: 1.8860, Test Loss: 1.8423\n",
            "Final, Train Loss: 1.9403, Test Loss: 1.8936\n",
            "Final, Train Loss: 2.2115, Test Loss: 1.8863\n",
            "Final, Train Loss: 1.8840, Test Loss: 1.8443\n",
            "Final, Train Loss: 0.0485, Test Loss: 0.4329\n",
            "Final, Train Loss: 2.3061, Test Loss: 2.3134\n",
            "Final, Train Loss: 2.3068, Test Loss: 2.3024\n",
            "Final, Train Loss: 1.8500, Test Loss: 1.7982\n",
            "Final, Train Loss: 2.3060, Test Loss: 2.3014\n",
            "Final, Train Loss: 0.0004, Test Loss: 0.0759\n",
            "Final, Train Loss: 0.0010, Test Loss: 0.0686\n",
            "Final, Train Loss: 0.0089, Test Loss: 0.1979\n",
            "Final, Train Loss: 0.0528, Test Loss: 0.2392\n",
            "Final, Train Loss: 2.3055, Test Loss: 2.3017\n",
            "Final, Train Loss: 2.3068, Test Loss: 2.3022\n",
            "Final, Train Loss: 2.3056, Test Loss: 2.3007\n",
            "Final, Train Loss: 2.3065, Test Loss: 2.3019\n",
            "Final, Train Loss: 2.3059, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.1993, Test Loss: 0.3256\n",
            "Final, Train Loss: 2.3065, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3067, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3062, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3057, Test Loss: 2.3015\n",
            "Final, Train Loss: 1.8146, Test Loss: 1.8264\n",
            "Final, Train Loss: 2.3054, Test Loss: 2.3013\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3014\n",
            "Final, Train Loss: 0.0381, Test Loss: 0.3237\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0781\n",
            "Final, Train Loss: 0.0155, Test Loss: 0.2622\n",
            "Final, Train Loss: 2.3056, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0959\n",
            "Final, Train Loss: 1.6858, Test Loss: 1.6300\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1007\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0976\n",
            "Final, Train Loss: 1.7672, Test Loss: 1.8455\n",
            "Final, Train Loss: 0.0457, Test Loss: 0.3260\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0865\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0865\n",
            "Final, Train Loss: 0.0334, Test Loss: 0.2698\n",
            "Final, Train Loss: 2.3071, Test Loss: 2.3019\n",
            "Final, Train Loss: 2.3055, Test Loss: 2.3015\n",
            "Final, Train Loss: 1.9705, Test Loss: 1.9073\n",
            "Final, Train Loss: 2.3057, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.2766, Test Loss: 0.4695\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0831\n",
            "Final, Train Loss: 2.3079, Test Loss: 2.3020\n",
            "Final, Train Loss: 2.3051, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.2554, Test Loss: 2.2808\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.0768\n",
            "Final, Train Loss: 2.3062, Test Loss: 2.3014\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3010\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0002, Test Loss: 0.0812\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.2136\n",
            "Final, Train Loss: 0.0645, Test Loss: 0.3335\n",
            "Final, Train Loss: 0.0008, Test Loss: 0.0701\n",
            "Final, Train Loss: 1.8819, Test Loss: 1.8125\n",
            "Final, Train Loss: 2.3062, Test Loss: 2.3019\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3014\n",
            "Final, Train Loss: 2.3054, Test Loss: 2.3009\n",
            "Final, Train Loss: 0.0002, Test Loss: 0.0795\n",
            "Final, Train Loss: 0.0002, Test Loss: 0.2028\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3055, Test Loss: 2.3005\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1440\n",
            "Final, Train Loss: 1.7877, Test Loss: 1.8054\n",
            "Final, Train Loss: 0.0352, Test Loss: 0.4485\n",
            "Final, Train Loss: 1.3372, Test Loss: 1.4230\n",
            "Final, Train Loss: 0.0391, Test Loss: 0.3661\n",
            "Final, Train Loss: 1.8527, Test Loss: 1.8911\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1642\n",
            "Final, Train Loss: 0.0002, Test Loss: 0.0781\n",
            "Final, Train Loss: 2.8307, Test Loss: 1.6199\n",
            "Final, Train Loss: 0.0002, Test Loss: 0.0803\n"
          ]
        }
      ],
      "source": [
        "# We first run and store N trials\n",
        "N=200\n",
        "N_trials = []\n",
        "\n",
        "\n",
        "for _ in range(N):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model = MLP()\n",
        "    model = model.to(device)\n",
        "    optimizer = NosAdam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Store parameters, train and test error\n",
        "    N_trials.append([beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031_uIdTsEhC",
      "metadata": {
        "id": "031_uIdTsEhC"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"N_trials_nosadam_mlp.pth\", \"wb\") as fp:\n",
        "  pickle.dump(N_trials, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72811ee-a1d8-408c-b3a5-e23b9b055bec",
      "metadata": {
        "id": "f72811ee-a1d8-408c-b3a5-e23b9b055bec"
      },
      "source": [
        "###### Perform bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059f03aa-7234-4943-a858-7e68e84e33d2",
      "metadata": {
        "id": "059f03aa-7234-4943-a858-7e68e84e33d2"
      },
      "outputs": [],
      "source": [
        "means_train = []\n",
        "means_test  = []\n",
        "# N_trials = N_trials.to('cpu')\n",
        "# Do the following 100 times :\n",
        "for _ in range(100):\n",
        "    # Resample N samples from the N-trials with replacement\n",
        "    N_sampled_indices = np.random.choice(list(range(len(N_trials))), N) # choose random indices in the list of N trials\n",
        "    \n",
        "    # Recover the lists associated to the indices and keep only intersting information, i.e. test and train errors\n",
        "    N_sampled_train_error = np.array([N_trials[i][4].cpu().detach().numpy() for i in N_sampled_indices])\n",
        "    N_sampled_test_error = np.array([N_trials[i][5].cpu().detach().numpy() for i in N_sampled_indices])\n",
        "    \n",
        "    # Compute statistic on the first K trials of the resampled dataset\n",
        "    means_train.append(N_sampled_train_error[:K].mean())\n",
        "    means_test.append(N_sampled_test_error[:K].mean())\n",
        "    \n",
        "# 5th percentile, 95 percentile of bootrap distribution\n",
        "fifth_percentile_train = np.percentile(means_train, 5)\n",
        "fifth_percentile_test = np.percentile(means_test, 5)\n",
        "\n",
        "ninety_fifth_percentile_train = np.percentile(means_train, 95)\n",
        "ninety_fifth_percentile_test = np.percentile(means_test, 95)\n",
        "\n",
        "# For plotting purposes only\n",
        "mean_all_train = np.array(means_train).mean()\n",
        "mean_all_test = np.array(means_test).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dfb7161-d1a2-4c08-be7e-32ff34fe3be0",
      "metadata": {
        "id": "9dfb7161-d1a2-4c08-be7e-32ff34fe3be0"
      },
      "source": [
        "###### Generate plots for mean error bars for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d78e184-b512-4f56-83e8-bd0bc6b927df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "9d78e184-b512-4f56-83e8-bd0bc6b927df",
        "outputId": "f085da32-76ad-46e7-ad77-3ce661277572"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train plot, each index in x will be a different optimizer and y its values\n",
        "plt.style.use('seaborn-white')\n",
        "plt.scatter(x=[0], y=[mean_all_train])\n",
        "plt.errorbar(x=[0], y=[mean_all_train], yerr=[[fifth_percentile_train],[ninety_fifth_percentile_train]], ecolor='red', color='black')\n",
        "plt.ylabel('Train Loss')\n",
        "plt.savefig('Train_nosadam_mlp.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b96b7ad-c939-497b-8ab2-a31364ded430",
      "metadata": {
        "id": "4b96b7ad-c939-497b-8ab2-a31364ded430"
      },
      "source": [
        "###### Generate plots for mean error bars for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311df6f3-7d51-4b53-bc91-dae7c5ddff15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "311df6f3-7d51-4b53-bc91-dae7c5ddff15",
        "outputId": "50056566-4f25-4652-c8c4-489178622e32"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train plot, each index in x will be a different optimizer and y its values\n",
        "plt.style.use('seaborn-white')\n",
        "plt.scatter(x=[0], y=[mean_all_test])\n",
        "plt.errorbar(x=[0], y=[mean_all_test], yerr=[[fifth_percentile_test],[ninety_fifth_percentile_test]], ecolor='red', color='black')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.savefig('Test_nosadam_mlp.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBRfSh49xasS",
      "metadata": {
        "id": "DBRfSh49xasS"
      },
      "source": [
        "### 4.3. Tuning Adam for the CNN on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VuStHJoqzl5Y",
      "metadata": {
        "id": "VuStHJoqzl5Y"
      },
      "source": [
        "##### Set up model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "FfnUAu5xzl5Z",
      "metadata": {
        "id": "FfnUAu5xzl5Z"
      },
      "outputs": [],
      "source": [
        "# Model fixed parameters\n",
        "model = CNN()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device) # good loss function for classification tasks\n",
        "num_epoch = 50\n",
        "size_minibatch = 128\n",
        "\n",
        "x_train = features_train\n",
        "y_train = labels_train\n",
        "x_test = features_test\n",
        "y_test = labels_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t1eNEdgdzl5a",
      "metadata": {
        "id": "t1eNEdgdzl5a",
        "tags": []
      },
      "source": [
        "##### Tune to find best parameter\n",
        "We perform trials until we have K of them, then we pick the best based on our statistic of interest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSu9ppsnzl5b",
      "metadata": {
        "id": "MSu9ppsnzl5b"
      },
      "source": [
        "#### 4.3.1. Initial search for best hyperparameters for **Adam** optimizer on **CNN**. K= 100. \n",
        "Interrupted because of nan loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oyss_Dmczl5b",
      "metadata": {
        "id": "oyss_Dmczl5b"
      },
      "source": [
        "##### Set up parameters and search space for the initial trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f6umyX4jzl5c",
      "metadata": {
        "id": "f6umyX4jzl5c"
      },
      "outputs": [],
      "source": [
        "N = 200\n",
        "K = 100 # Number of trials being kept for the statistic\n",
        "\n",
        "# Initial search spaces for parameters\n",
        "alpha_0 = np.linspace(10**(-2), 10**(4), N)\n",
        "beta_1 = np.linspace(10**(-3), 1, N)\n",
        "beta_2 = np.linspace(10**(-4), 1, N)\n",
        "eps = np.linspace(10**(-10), 10**(10), N)\n",
        "\n",
        "# TODO: tune number of decay steps between 0.5 and 1 times the number of training steps\n",
        "# TODO : tune learning rate decay factor within 10**-3, 10**-2, 10**-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4yWE5lNhzl5d",
      "metadata": {
        "id": "4yWE5lNhzl5d"
      },
      "source": [
        "Perform search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "aMz4BHWrzl5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "aMz4BHWrzl5d",
        "outputId": "e871fff5-9418-4129-cd9d-82093370cad4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: 0.2715, Test Loss: 0.1980\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: 1954.3563, Test Loss: 2359.0366\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: 1335.0944, Test Loss: 865.0282\n",
            "Final, Train Loss: nan, Test Loss: nan\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f4010c53a96c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a138ffa15448>\u001b[0m in \u001b[0;36mrun_nn\u001b[0;34m(x_train, y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e196faf1b541>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "nb_hyperamaters_to_tune = 4\n",
        "nb_exported_statistics  = 2\n",
        "\n",
        "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
        "\n",
        "for _ in range(K):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model=CNN()\n",
        "    model=model.to(device)\n",
        "    optimizer = NosAdam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    # train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch)\n",
        "    \n",
        "    \n",
        "    # Concatenate hyperparameters with results\n",
        "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
        "    \n",
        "    # Check wether we have the smallest test error and store parameters in case we find it\n",
        "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
        "        lowest_test_error = vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G97NRkQ70yBX",
      "metadata": {
        "id": "G97NRkQ70yBX"
      },
      "source": [
        "#### 4.3.2. Final search for best hyperparameters for **Adam** optimizer on **CNN**. K = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BcWL5tIe0yBY",
      "metadata": {
        "id": "BcWL5tIe0yBY"
      },
      "source": [
        "##### Set up parameters and search space for the final trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2LHRX7mT0yBY",
      "metadata": {
        "id": "2LHRX7mT0yBY"
      },
      "outputs": [],
      "source": [
        "N = 200\n",
        "K = 50 # Number of trials being kept for the statistic\n",
        "\n",
        "# Final search spaces for parameters\n",
        "alpha_0 = np.linspace(10**(-1), 10, N)\n",
        "beta_1 = np.linspace(10**(-3), 1, N)\n",
        "beta_2 = np.linspace(10**(-4), 1, N)\n",
        "eps = np.linspace(10**(-6), 10**(-2), N)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VSruqZFS0yBZ",
      "metadata": {
        "id": "VSruqZFS0yBZ"
      },
      "source": [
        "Perform search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Epa_A6zV0yBZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epa_A6zV0yBZ",
        "outputId": "e5ed770a-04cc-42a7-da88-8a6c79c8f672"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final, Train Loss: 0.0000, Test Loss: 0.0315\n",
            "Final, Train Loss: 2.3064, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.1972, Test Loss: 0.0344\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0566\n",
            "Final, Train Loss: 2.3057, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3049, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3062, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.3060, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3049, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2152\n",
            "Final, Train Loss: 0.0025, Test Loss: 0.1179\n",
            "Final, Train Loss: 2.3072, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.0046, Test Loss: 0.1936\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0393\n",
            "Final, Train Loss: 2.3069, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0466\n",
            "Final, Train Loss: 2.3057, Test Loss: 2.3021\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.8203, Test Loss: 0.3002\n",
            "Final, Train Loss: 2.3070, Test Loss: 2.3019\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0326\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.1023, Test Loss: 0.2069\n"
          ]
        }
      ],
      "source": [
        "nb_hyperamaters_to_tune = 4\n",
        "nb_exported_statistics  = 2\n",
        "\n",
        "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
        "\n",
        "\n",
        "for _ in range(K):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model=CNN()\n",
        "    model=model.to(device)\n",
        "    optimizer = NosAdam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Concatenate hyperparameters with results\n",
        "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
        "    \n",
        "    # Check wether we have the smallest test error and store parameters in case we find it\n",
        "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
        "        lowest_test_error = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "toonHfIXQAJe",
      "metadata": {
        "id": "toonHfIXQAJe"
      },
      "outputs": [],
      "source": [
        "# Print best parameters\n",
        "\n",
        "print('Beta 1: %.2f' % lowest_test_error[0])\n",
        "print('Beta 2: %.2f' % lowest_test_error[1])\n",
        "print('Epsilon: %.2e' % lowest_test_error[2])\n",
        "print('Learning rate: %.2f' % lowest_test_error[3])\n",
        "print('Train error: %.6f' % lowest_test_error[4])\n",
        "print('Test error: %.4f' % lowest_test_error[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mbJtvIFv0yBd",
      "metadata": {
        "id": "mbJtvIFv0yBd",
        "tags": []
      },
      "source": [
        "##### 4.3.3. Estimating trial outcomes via bootstrap\n",
        "At this stage we want to estimate means and uncertainties of our tuning protocol"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IohNC8cu0yBd",
      "metadata": {
        "id": "IohNC8cu0yBd"
      },
      "source": [
        "###### Run N trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AYysfct4QDzs",
      "metadata": {
        "id": "AYysfct4QDzs"
      },
      "outputs": [],
      "source": [
        "# We first run and store N trials\n",
        "N=200\n",
        "N_trials = []\n",
        "model = CNN()\n",
        "model = model.to(device)\n",
        "\n",
        "for _ in range(N):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = mlp_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Store parameters, train and test error\n",
        "    N_trials.append([beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QJjhEOxk0yBf",
      "metadata": {
        "id": "QJjhEOxk0yBf"
      },
      "source": [
        "###### Perform bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-SqLoaXH0yBf",
      "metadata": {
        "id": "-SqLoaXH0yBf"
      },
      "outputs": [],
      "source": [
        "means_train = []\n",
        "means_test  = []\n",
        "# Do the following 100 times :\n",
        "for _ in range(100):\n",
        "    # Resample N samples from the N-trials with replacement\n",
        "    N_sampled_indices = np.random.choice(list(range(len(N_trials))), N) # choose random indices in the list of N trials\n",
        "    \n",
        "    # Recover the lists associated to the indices and keep only intersting information, i.e. test and train errors\n",
        "    N_sampled_train_error = np.array([N_trials[i][4] for i in N_sampled_indices])\n",
        "    N_sampled_test_error = np.array([N_trials[i][5] for i in N_sampled_indices])\n",
        "    \n",
        "    # Compute statistic on the first K trials of the resampled dataset\n",
        "    means_train.append(N_sampled_train_error[:K].mean())\n",
        "    means_test.append(N_sampled_test_error[:K].mean())\n",
        "    \n",
        "# 5th percentile, 95 percentile of bootrap distribution\n",
        "fifth_percentile_train = np.percentile(means_train, 5)\n",
        "fifth_percentile_test = np.percentile(means_test, 5)\n",
        "\n",
        "ninety_fifth_percentile_train = np.percentile(means_train, 95)\n",
        "ninety_fifth_percentile_test = np.percentile(means_test, 95)\n",
        "\n",
        "# For plotting purposes only\n",
        "mean_all_train = np.array(means_train).mean()\n",
        "mean_all_test = np.array(means_test).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cSP6WEa60yBg",
      "metadata": {
        "id": "cSP6WEa60yBg"
      },
      "source": [
        "###### Generate plots for mean error bars for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pmL4mKlYQHCO",
      "metadata": {
        "id": "pmL4mKlYQHCO"
      },
      "outputs": [],
      "source": [
        "# Train plot, each index in x will be a different optimizer and y its values\n",
        "plt.scatter(x=[0], y=[mean_all_train])\n",
        "plt.errorbar(x=[0], y=[mean_all_train], yerr=[[fifth_percentile_train],[ninety_fifth_percentile_train]], ecolor='red', color='black')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p70_yxIt0yBh",
      "metadata": {
        "id": "p70_yxIt0yBh"
      },
      "source": [
        "###### Generate plots for mean error bars for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o1S5wAU5QJTX",
      "metadata": {
        "id": "o1S5wAU5QJTX"
      },
      "outputs": [],
      "source": [
        "# Train plot, each index in x will be a different optimizer and y its values\n",
        "plt.scatter(x=[0], y=[mean_all_test])\n",
        "plt.errorbar(x=[0], y=[mean_all_test], yerr=[[fifth_percentile_test],[ninety_fifth_percentile_test]], ecolor='red', color='black')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "nosadam_mlp_cnn.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a31065c2cb285fc89af45a6e0e00b20f2f53e470cc5659f0bf6d768d5279787d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 ('knvenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
