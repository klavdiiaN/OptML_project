{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e359ffb-71a2-43db-87a3-4de42e63aa55",
   "metadata": {},
   "source": [
    "# 1.Imports & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a167aec-3ddd-4067-8b5f-77af4fe2854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Setup predictable randomization\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Setup CUda\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42f77e-92d5-4cb3-9099-b699d7c7858d",
   "metadata": {},
   "source": [
    "# 2. Loading and preparation of data\n",
    "As a basis for comparison we will be using the MNIST dataset. If we manage to do all the work we want, we will then use other datasets for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668867e-acbe-4676-84dc-1911b88c0729",
   "metadata": {},
   "source": [
    "### 2.1. Definition of methods to extract data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "febd210c-2240-45d1-8860-14be25f58c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, image_shape, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048581f-bdb6-4a22-9b0c-9f371c1303b3",
   "metadata": {},
   "source": [
    "### 2.2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c71149-8c1e-4fbd-b2b9-2e332fcf8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "data_folder = 'mnist_data'\n",
    "\n",
    "train_images_path = os.path.join(data_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels = extract_labels(test_labels_path, test_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8b6f8-0472-4936-a88d-cde07e2359b4",
   "metadata": {},
   "source": [
    "### 2.3. Convert data from numpy arrays to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b5bfe7-685e-4adc-8a48-fc1a25a39381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: torch.Size([60000, 28, 28]) \n",
      "Testing features: torch.Size([10000, 28, 28])\n",
      "Training labels: torch.Size([60000]) \n",
      "Testing labels: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "features_train=torch.from_numpy(train_images)#.to(device)\n",
    "features_test=torch.from_numpy(test_images)#.to(device)\n",
    "print('Training features:', features_train.shape, '\\n'\n",
    "'Testing features:', features_test.shape)\n",
    "\n",
    "labels_train=torch.from_numpy(train_labels)#.to(device)\n",
    "labels_test=torch.from_numpy(test_labels)#.to(device)\n",
    "print('Training labels:', labels_train.shape, '\\n'\n",
    "'Testing labels:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072269a3-e3f6-470c-8ca9-58a0555d9851",
   "metadata": {},
   "source": [
    "### 2.4. Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24f2cbd-0279-44d6-89d2-1dca355248df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = features_train.float().mean(), features_train.float().std()\n",
    "\n",
    "features_train = features_train.float().sub_(mean).div_(std)\n",
    "features_test = features_test.float().sub_(mean).div_(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc94097-21c0-4308-ba10-af4dc6d04f30",
   "metadata": {},
   "source": [
    "# 3. Setting up network and evaluation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eeb387-7739-466e-972b-3f10a9a358a7",
   "metadata": {},
   "source": [
    "### 3.1. Multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc500d-e32d-401a-84a3-cc4f65359be0",
   "metadata": {},
   "source": [
    "##### 3.1.1. Defining class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13013745-d51c-4348-9f26-211b04d3cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size_1=512, hidden_size_2=100, hidden_size_3=10):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, hidden_size_1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size_1, hidden_size_2))\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(hidden_size_2, hidden_size_3)) #maybe try more layers\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119d2d9-562a-4bdf-8ce7-eea9d59272fa",
   "metadata": {},
   "source": [
    "##### 3.1.2. Implementation of method for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e5ca3dd-313e-4c15-99da-10937d42595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_nn(x_train, y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch):\n",
    "    loss_all_train, loss_all_test = [], []\n",
    "    loss_train_ret = 0\n",
    "    loss_test_ret = 0\n",
    "    loss_train = 0\n",
    "    epochs_all = torch.arange(1, num_epoch+num_epoch/10, num_epoch/10)\n",
    "    epochs_all[-1] = num_epoch - 1\n",
    "            \n",
    "    for epoch in range(num_epoch):\n",
    "        for b in range(0, x_train.size(0), size_minibatch):\n",
    "            y = model(x_train[b:b+size_minibatch])\n",
    "            loss_train = criterion(y, y_train[b:b+size_minibatch])\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch == num_epoch - 1:\n",
    "            loss_train = loss_train.detach().numpy()\n",
    "            loss_all_train.append(loss_train)\n",
    "\n",
    "            y_test_obt = model(x_test)\n",
    "            loss_test = criterion(y_test_obt, y_test)\n",
    "            loss_test = loss_test.detach().numpy()\n",
    "            loss_all_test.append(loss_test)\n",
    "            \n",
    "            loss_train_ret = loss_train\n",
    "            loss_test_ret = loss_test\n",
    "            \n",
    "            print('Final, Train Loss: %.4f, Test Loss: %.4f' %(loss_train, loss_test))\n",
    "\n",
    "    return loss_train_ret, loss_test_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93565277-6e32-47ca-8c17-55098b57d7bb",
   "metadata": {},
   "source": [
    "# 5. Metrics of our tuning protocol\n",
    "At this stage, we want to select the hyperparameter search space for each optimizer. This way, we can first tune the hyperparameters of each optimizer separately and then select the trial that achieved lowest final validation error.\n",
    "We then comapre the optimizers' performance by looking at the validation and test errors as suggested in the paper \"On empirical comparisons of optimizers for deep learning\".\n",
    "\n",
    "We will also look at the training speed (number of training steps required) to reach a traget validation error.\n",
    "\n",
    "Everything is tuned on a log scale.\n",
    "\n",
    "No L_2 regularization or weight decay is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b4d1e-5dbb-4a8a-a365-eb2ce1ecafc7",
   "metadata": {},
   "source": [
    "### 5.1. Tuning protocol using bootstrap\n",
    "To estimate means and uncertainties of our tuning protocol we will use bootstrapping starting from an initial search space suggested by the paper \"On Empirical Comparisons of Optimizers for Deep Learning\".\n",
    "We run N trials by randomly picking values in the search space of the algorithm at every trial.\n",
    "Then we sample these trials with replacement and compute our statistic on the first K trials of this sample. We repeat this process 100 times and compute the 5th percentile and 95th percentile of the bootstrap distribution.\n",
    "\n",
    "This allows us to plot the error bars to show the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a304d9-db74-4bc5-a12c-b97b971097bd",
   "metadata": {},
   "source": [
    "### 5.2. Tuning Adam for a CNN on MNIST\n",
    "The hyperparameters we are tuning are alpha_0/epsilon, 1 - beta_1, 1 - beta_2, epsilon.\n",
    "The initial search spaces are suggested based on the experience of the writers of the same paper, \"On empirical comparisons of optimizers for deep learning\".\n",
    "N is also suggested to be 500 and K to be 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fee583-15df-49be-8132-5d00b6b01366",
   "metadata": {},
   "source": [
    "##### 5.2.1. Set up parameters and search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f6a27da-bf06-430a-810e-773c4faa93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: We used the final search spaces instead of the initial ones, should we reproduce the whole method\n",
    "\n",
    "N = 2 # Number of trials\n",
    "K = 1 # Number of trials being kept for the statistic\n",
    "\n",
    "# Search spaces for parameters\n",
    "alpha_0 = np.linspace(10**(-1), 10, N)\n",
    "beta_1 = np.linspace(10**(-3), 1, N)\n",
    "beta_2 = np.linspace(10**(-4), 1, N)\n",
    "eps = np.linspace(10**(-6), 10**(-2), N)\n",
    "\n",
    "# TODO: tune number of decay steps between 0.5 and 1 times the number of training steps\n",
    "# TODO : tune learning rate decay factor within 10**-3, 10**-2, 10**-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c3fd8-c425-4ce5-991a-0ad3cd2ba426",
   "metadata": {},
   "source": [
    "##### 5.2.2. Set up model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d84470-824e-4386-a781-c9df9bb78f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fixed parameters\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss() # good loss function for classification tasks\n",
    "num_epoch = 50\n",
    "size_minibatch = 128\n",
    "\n",
    "x_train = features_train\n",
    "y_train = labels_train\n",
    "x_test = features_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca30bc-c245-45ab-8d7e-ec9c37d62ab3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 5.2.2. Tune to find best parameter\n",
    "We perform trials until we have K of them, then we pick the best based on our statistic of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c167327-ee1a-42e9-b628-14f61022aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_hyperamaters_to_tune = 4\n",
    "nb_exported_statistics  = 2\n",
    "\n",
    "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(K):\n",
    "    # Pick random values from the intervals given for the different parameters\n",
    "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
    "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
    "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
    "    eps_pick      = float(np.random.choice(eps, 1))\n",
    "    learning_rate = alpha_0_pick * eps_pick\n",
    "    \n",
    "    # Build optimizer from parameters\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
    "    \n",
    "    # Run\n",
    "    train_error, test_error = mlp_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
    "    \n",
    "    # Concatenate hyperparameters with results\n",
    "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
    "    \n",
    "    # Check wether we have the smallest test error and store parameters in case we find it\n",
    "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
    "        lowest_test_error = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a983b-b65e-4185-b56c-b6fabd784c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters\n",
    "\n",
    "print('Beta 1: %.2f' % lowest_test_error[0])\n",
    "print('Beta 2: %.2f' % lowest_test_error[1])\n",
    "print('Epsilon: %.2f' % lowest_test_error[2])\n",
    "print('Learning rate: %.2f' % lowest_test_error[3])\n",
    "print('Train error: %.2f' % lowest_test_error[4])\n",
    "print('Test error: %.2f' % lowest_test_error[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b6adf-dac0-4e2e-a3f4-27186b406140",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 5.2.3. Estimating trial outcomes via bootstrap\n",
    "At this stage we want to estimate means and uncertainties of our tuning protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db030739-278f-4fe8-9932-b4af662923ff",
   "metadata": {},
   "source": [
    "###### Run N trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad08fad-3e95-4d86-8d4d-38c2f43d6aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final, Train Loss: 1.9869, Test Loss: 2.1574\n",
      "Final, Train Loss: 1.9801, Test Loss: 2.1574\n"
     ]
    }
   ],
   "source": [
    "# We first run and store N trials\n",
    "N_trials = []\n",
    "\n",
    "for _ in range(N):\n",
    "    # Pick random values from the intervals given for the different parameters\n",
    "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
    "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
    "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
    "    eps_pick      = float(np.random.choice(eps, 1))\n",
    "    learning_rate = alpha_0_pick * eps_pick\n",
    "    \n",
    "    # Build optimizer from parameters\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
    "    \n",
    "    # Run\n",
    "    train_error, test_error = mlp_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
    "    \n",
    "    # Store parameters, train and test error\n",
    "    N_trials.append([beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72811ee-a1d8-408c-b3a5-e23b9b055bec",
   "metadata": {},
   "source": [
    "###### Perform bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "059f03aa-7234-4943-a858-7e68e84e33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_train = []\n",
    "means_test  = []\n",
    "# Do the following 100 times :\n",
    "for _ in range(100):\n",
    "    # Resample N samples from the N-trials with replacement\n",
    "    N_sampled_indices = np.random.choice(list(range(len(N_trials))), N) # choose random indices in the list of N trials\n",
    "    \n",
    "    # Recover the lists associated to the indices and keep only intersting information, i.e. test and train errors\n",
    "    N_sampled_train_error = np.array([N_trials[i][4] for i in N_sampled_indices])\n",
    "    N_sampled_test_error = np.array([N_trials[i][5] for i in N_sampled_indices])\n",
    "    \n",
    "    # Compute statistic on the first K trials of the resampled dataset\n",
    "    means_train.append(N_sampled_train_error[:K].mean())\n",
    "    means_test.append(N_sampled_test_error[:K].mean())\n",
    "    \n",
    "# 5th percentile, 95 percentile of bootrap distribution\n",
    "fifth_percentile_train = np.percentile(means_train, 5)\n",
    "fifth_percentile_test = np.percentile(means_test, 5)\n",
    "\n",
    "ninety_fifth_percentile_train = np.percentile(means_train, 95)\n",
    "ninety_fifth_percentile_test = np.percentile(means_test, 95)\n",
    "\n",
    "# For plotting purposes only\n",
    "mean_all_train = np.array(means_train).mean()\n",
    "mean_all_test = np.array(means_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb7161-d1a2-4c08-be7e-32ff34fe3be0",
   "metadata": {},
   "source": [
    "###### Generate plots for mean error bars for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78e184-b512-4f56-83e8-bd0bc6b927df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train plot, each index in x will be a different optimizer and y its values\n",
    "plt.scatter(x=[0], y=[mean_all_train])\n",
    "plt.errorbar(x=[0], y=[mean_all_train], yerr=[[fifth_percentile_train],[ninety_fifth_percentile_train]], ecolor='red', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96b7ad-c939-497b-8ab2-a31364ded430",
   "metadata": {},
   "source": [
    "###### Generate plots for mean error bars for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311df6f3-7d51-4b53-bc91-dae7c5ddff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train plot, each index in x will be a different optimizer and y its values\n",
    "plt.scatter(x=[0], y=[mean_all_test])\n",
    "plt.errorbar(x=[0], y=[mean_all_test], yerr=[[fifth_percentile_test],[ninety_fifth_percentile_test]], ecolor='red', color='black')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
