{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e359ffb-71a2-43db-87a3-4de42e63aa55",
      "metadata": {
        "id": "9e359ffb-71a2-43db-87a3-4de42e63aa55"
      },
      "source": [
        "# 1.Imports & environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a167aec-3ddd-4067-8b5f-77af4fe2854f",
      "metadata": {
        "id": "2a167aec-3ddd-4067-8b5f-77af4fe2854f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "import sys\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" #magic line to deal with my versions of libraries\n",
        "\n",
        "# Setup predictable randomization\n",
        "seed = 10\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Setup CUda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntWl8yGkkPox",
        "outputId": "69a3671e-0d6f-441a-c996-7cdf56ffb0cc"
      },
      "id": "ntWl8yGkkPox",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c42f77e-92d5-4cb3-9099-b699d7c7858d",
      "metadata": {
        "id": "8c42f77e-92d5-4cb3-9099-b699d7c7858d"
      },
      "source": [
        "# 2. Loading and preparation of data\n",
        "As a basis for comparison we will be using the MNIST dataset. If we manage to do all the work we want, we will then use other datasets for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3668867e-acbe-4676-84dc-1911b88c0729",
      "metadata": {
        "id": "3668867e-acbe-4676-84dc-1911b88c0729"
      },
      "source": [
        "### 2.1. Definition of methods to extract data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "febd210c-2240-45d1-8860-14be25f58c40",
      "metadata": {
        "id": "febd210c-2240-45d1-8860-14be25f58c40"
      },
      "outputs": [],
      "source": [
        "def extract_data(filename, image_shape, image_number):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        bytestream.read(16)\n",
        "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
        "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_labels(filename, image_number):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        bytestream.read(8)\n",
        "        buf = bytestream.read(1 * image_number)\n",
        "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a048581f-bdb6-4a22-9b0c-9f371c1303b3",
      "metadata": {
        "id": "a048581f-bdb6-4a22-9b0c-9f371c1303b3"
      },
      "source": [
        "### 2.2. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c71149-8c1e-4fbd-b2b9-2e332fcf8e1b",
      "metadata": {
        "id": "c7c71149-8c1e-4fbd-b2b9-2e332fcf8e1b"
      },
      "outputs": [],
      "source": [
        "image_shape = (28, 28)\n",
        "train_set_size = 60000\n",
        "test_set_size = 10000\n",
        "data_folder = 'mnist_data'\n",
        "\n",
        "train_images_path = os.path.join(data_folder, 'train-images-idx3-ubyte.gz')\n",
        "train_labels_path = os.path.join(data_folder, 'train-labels-idx1-ubyte.gz')\n",
        "test_images_path = os.path.join(data_folder, 't10k-images-idx3-ubyte.gz')\n",
        "test_labels_path = os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
        "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
        "train_labels = extract_labels(train_labels_path, train_set_size)\n",
        "test_labels = extract_labels(test_labels_path, test_set_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff8b6f8-0472-4936-a88d-cde07e2359b4",
      "metadata": {
        "id": "eff8b6f8-0472-4936-a88d-cde07e2359b4"
      },
      "source": [
        "### 2.3. Convert data from numpy arrays to torch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b5bfe7-685e-4adc-8a48-fc1a25a39381",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3b5bfe7-685e-4adc-8a48-fc1a25a39381",
        "outputId": "56b0354c-57b7-48cc-ba12-12721d364d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features: torch.Size([60000, 28, 28]) \n",
            "Testing features: torch.Size([10000, 28, 28])\n",
            "Training labels: torch.Size([60000]) \n",
            "Testing labels: torch.Size([10000])\n"
          ]
        }
      ],
      "source": [
        "features_train=torch.from_numpy(train_images).to(device)\n",
        "features_test=torch.from_numpy(test_images).to(device)\n",
        "print('Training features:', features_train.shape, '\\n'\n",
        "'Testing features:', features_test.shape)\n",
        "\n",
        "labels_train=torch.from_numpy(train_labels).to(device)\n",
        "labels_test=torch.from_numpy(test_labels).to(device)\n",
        "print('Training labels:', labels_train.shape, '\\n'\n",
        "'Testing labels:', labels_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072269a3-e3f6-470c-8ca9-58a0555d9851",
      "metadata": {
        "id": "072269a3-e3f6-470c-8ca9-58a0555d9851"
      },
      "source": [
        "### 2.4. Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24f2cbd-0279-44d6-89d2-1dca355248df",
      "metadata": {
        "id": "d24f2cbd-0279-44d6-89d2-1dca355248df"
      },
      "outputs": [],
      "source": [
        "mean, std = features_train.float().mean(), features_train.float().std()\n",
        "\n",
        "features_train = features_train.float().sub_(mean).div_(std)\n",
        "features_test = features_test.float().sub_(mean).div_(std)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape to make the 1st channel be a batch size (important for CNN)\n",
        "\n",
        "features_train = features_train.reshape(-1, 1, 28, 28)\n",
        "features_test = features_test.reshape(-1, 1, 28, 28)"
      ],
      "metadata": {
        "id": "gO2vsS-k6GY9"
      },
      "id": "gO2vsS-k6GY9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bdc94097-21c0-4308-ba10-af4dc6d04f30",
      "metadata": {
        "id": "bdc94097-21c0-4308-ba10-af4dc6d04f30"
      },
      "source": [
        "# 3. Setting up networks and evaluation methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93eeb387-7739-466e-972b-3f10a9a358a7",
      "metadata": {
        "id": "93eeb387-7739-466e-972b-3f10a9a358a7"
      },
      "source": [
        "### 3.1. Multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13013745-d51c-4348-9f26-211b04d3cc1d",
      "metadata": {
        "id": "13013745-d51c-4348-9f26-211b04d3cc1d"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_size_1=512, hidden_size_2=100, hidden_size_3=10):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, hidden_size_1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_size_1, hidden_size_2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_size_2, hidden_size_3))\n",
        "    \n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "mlp = MLP()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2. Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "g3hZg487lszk"
      },
      "id": "g3hZg487lszk"
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "   def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=5, stride=1, padding='same')\n",
        "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=5, stride=1, padding='same')\n",
        "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "   def forward(self, input:torch.Tensor) -> torch.Tensor:\n",
        "        pool1 = torch.max_pool2d(F.relu(self.conv1(input)), kernel_size=2, stride=2)\n",
        "        pool2 = torch.max_pool2d(F.relu(self.conv2(pool1)), kernel_size=2, stride=2)\n",
        "        res = pool2.reshape(-1, 64*7*7)\n",
        "        hidden = F.relu(self.fc1(res))\n",
        "        output = self.fc2(hidden)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "yUu42pH8l5md"
      },
      "id": "yUu42pH8l5md",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e119d2d9-562a-4bdf-8ce7-eea9d59272fa",
      "metadata": {
        "id": "e119d2d9-562a-4bdf-8ce7-eea9d59272fa"
      },
      "source": [
        "### 3.3. Implementation of method for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5ca3dd-313e-4c15-99da-10937d42595c",
      "metadata": {
        "id": "8e5ca3dd-313e-4c15-99da-10937d42595c"
      },
      "outputs": [],
      "source": [
        "def run_nn(x_train, y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch):\n",
        "    # loss_all_train, loss_all_test = [], []\n",
        "    loss_train_ret = 0\n",
        "    loss_test_ret = 0\n",
        "    loss_train = 0\n",
        "    # epochs_all = torch.arange(1, num_epoch+num_epoch/10, num_epoch/10)\n",
        "    # epochs_all[-1] = num_epoch - 1\n",
        "            \n",
        "    for epoch in range(num_epoch):\n",
        "        for b in range(0, x_train.size(0), size_minibatch):\n",
        "            # y = model(x_train.narrow(0, b, size_minibatch))\n",
        "            # loss_train = criterion(y, y_train.narrow(0, b, size_minibatch))\n",
        "            y = model(x_train[b:b+size_minibatch])\n",
        "            loss_train = criterion(y, y_train[b:b+size_minibatch])\n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch == num_epoch - 1:\n",
        "            # loss_train = loss_train.detach().numpy()\n",
        "            # loss_all_train.append(loss_train)\n",
        "\n",
        "            y_test_obt = model(x_test)\n",
        "            loss_test = criterion(y_test_obt, y_test)\n",
        "            # loss_test = loss_test.detach().numpy()\n",
        "            # loss_all_test.append(loss_test)\n",
        "            \n",
        "            loss_train_ret = loss_train\n",
        "            loss_test_ret = loss_test\n",
        "            \n",
        "            print('Final, Train Loss: %.4f, Test Loss: %.4f' %(loss_train, loss_test))\n",
        "\n",
        "    return loss_train_ret, loss_test_ret"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93565277-6e32-47ca-8c17-55098b57d7bb",
      "metadata": {
        "id": "93565277-6e32-47ca-8c17-55098b57d7bb"
      },
      "source": [
        "# 4. Metrics of our tuning protocol\n",
        "At this stage, we want to select the hyperparameter search space for each optimizer. This way, we can first tune the hyperparameters of each optimizer separately and then select the trial that achieved lowest final validation error.\n",
        "We then comapre the optimizers' performance by looking at the validation and test errors as suggested in the paper \"On empirical comparisons of optimizers for deep learning\".\n",
        "\n",
        "We will also look at the training speed (number of training steps required) to reach a traget validation error.\n",
        "\n",
        "Everything is tuned on a log scale.\n",
        "\n",
        "No L_2 regularization or weight decay is used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "124b4d1e-5dbb-4a8a-a365-eb2ce1ecafc7",
      "metadata": {
        "id": "124b4d1e-5dbb-4a8a-a365-eb2ce1ecafc7"
      },
      "source": [
        "### 4.1. Tuning protocol using bootstrap\n",
        "To estimate means and uncertainties of our tuning protocol we will use bootstrapping starting from an initial search space suggested by the paper \"On Empirical Comparisons of Optimizers for Deep Learning\".\n",
        "We run N trials by randomly picking values in the search space of the algorithm at every trial.\n",
        "Then we sample these trials with replacement and compute our statistic on the first K trials of this sample. We repeat this process 100 times and compute the 5th percentile and 95th percentile of the bootstrap distribution.\n",
        "\n",
        "This allows us to plot the error bars to show the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a304d9-db74-4bc5-a12c-b97b971097bd",
      "metadata": {
        "id": "92a304d9-db74-4bc5-a12c-b97b971097bd"
      },
      "source": [
        "### 4.2. Tuning Adam for the MLP on MNIST\n",
        "The hyperparameters we are tuning are alpha_0/epsilon, 1 - beta_1, 1 - beta_2, epsilon.\n",
        "The initial search spaces are suggested based on the experience of the writers of the same paper, \"On empirical comparisons of optimizers for deep learning\".\n",
        "N is also suggested to be 500 and K to be 100."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "465c3fd8-c425-4ce5-991a-0ad3cd2ba426",
      "metadata": {
        "id": "465c3fd8-c425-4ce5-991a-0ad3cd2ba426"
      },
      "source": [
        "##### Set up model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d84470-824e-4386-a781-c9df9bb78f10",
      "metadata": {
        "id": "52d84470-824e-4386-a781-c9df9bb78f10"
      },
      "outputs": [],
      "source": [
        "# Model fixed parameters\n",
        "model = MLP()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device) # good loss function for classification tasks\n",
        "num_epoch = 50\n",
        "size_minibatch = 128\n",
        "\n",
        "x_train = features_train\n",
        "y_train = labels_train\n",
        "x_test = features_test\n",
        "y_test = labels_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ca30bc-c245-45ab-8d7e-ec9c37d62ab3",
      "metadata": {
        "tags": [],
        "id": "25ca30bc-c245-45ab-8d7e-ec9c37d62ab3"
      },
      "source": [
        "##### Tune to find best parameter\n",
        "We perform trials until we have K of them, then we pick the best based on our statistic of interest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1. Initial search for best hyperparameters for **Adam** optimizer on **MLP**. K= 100. \n",
        "Interrupted because of nan loss."
      ],
      "metadata": {
        "id": "mGYqlhu0eJwA"
      },
      "id": "mGYqlhu0eJwA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Set up parameters and search space for the initial trial"
      ],
      "metadata": {
        "id": "0pqeK5opwRvk"
      },
      "id": "0pqeK5opwRvk"
    },
    {
      "cell_type": "code",
      "source": [
        "N = 200\n",
        "K = 100 # Number of trials being kept for the statistic\n",
        "\n",
        "# Initial search spaces for parameters\n",
        "alpha_0 = np.linspace(10**(-2), 10**(4), N)\n",
        "beta_1 = np.linspace(10**(-3), 1, N)\n",
        "beta_2 = np.linspace(10**(-4), 1, N)\n",
        "eps = np.linspace(10**(-10), 10**(10), N)\n",
        "\n",
        "# TODO: tune number of decay steps between 0.5 and 1 times the number of training steps\n",
        "# TODO : tune learning rate decay factor within 10**-3, 10**-2, 10**-1"
      ],
      "metadata": {
        "id": "FhGg_EqiwWKJ"
      },
      "id": "FhGg_EqiwWKJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform search"
      ],
      "metadata": {
        "id": "W09RkUufwxAx"
      },
      "id": "W09RkUufwxAx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c167327-ee1a-42e9-b628-14f61022aef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5c167327-ee1a-42e9-b628-14f61022aef4",
        "outputId": "6d2265bf-ca19-4229-b80d-212451de5ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: 0.4954, Test Loss: 0.4477\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n",
            "Final, Train Loss: nan, Test Loss: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8bc99d7bba9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Concatenate hyperparameters with results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-799746aece03>\u001b[0m in \u001b[0;36mrun_nn\u001b[0;34m(x_train, y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "nb_hyperamaters_to_tune = 4\n",
        "nb_exported_statistics  = 2\n",
        "\n",
        "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
        "\n",
        "for _ in range(K):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model=MLP()\n",
        "    model=model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Concatenate hyperparameters with results\n",
        "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
        "    \n",
        "    # Check wether we have the smallest test error and store parameters in case we find it\n",
        "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
        "        lowest_test_error = vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2. Final search for best hyperparameters for **Adam** optimizer on **MLP**. K = 50"
      ],
      "metadata": {
        "id": "6yOLXHgin-Ve"
      },
      "id": "6yOLXHgin-Ve"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Set up parameters and search space for the final trial"
      ],
      "metadata": {
        "id": "WYEeVzSvw-fz"
      },
      "id": "WYEeVzSvw-fz"
    },
    {
      "cell_type": "code",
      "source": [
        "N = 200\n",
        "K = 50 # Number of trials being kept for the statistic\n",
        "\n",
        "# Final search spaces for parameters\n",
        "alpha_0 = np.linspace(10**(-1), 10, N)\n",
        "beta_1 = np.linspace(10**(-3), 1, N)\n",
        "beta_2 = np.linspace(10**(-4), 1, N)\n",
        "eps = np.linspace(10**(-6), 10**(-2), N)"
      ],
      "metadata": {
        "id": "4jUNhWkVv96z"
      },
      "id": "4jUNhWkVv96z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform search"
      ],
      "metadata": {
        "id": "4gXEoY3nxDaO"
      },
      "id": "4gXEoY3nxDaO"
    },
    {
      "cell_type": "code",
      "source": [
        "nb_hyperamaters_to_tune = 4\n",
        "nb_exported_statistics  = 2\n",
        "\n",
        "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
        "\n",
        "\n",
        "for _ in range(K):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model = MLP()\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Concatenate hyperparameters with results\n",
        "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
        "    \n",
        "    # Check wether we have the smallest test error and store parameters in case we find it\n",
        "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
        "        lowest_test_error = vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA5uFjwh2-V8",
        "outputId": "b4d877f0-78da-4fbe-bb34-91a380f2921d"
      },
      "id": "RA5uFjwh2-V8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final, Train Loss: 0.1809, Test Loss: 0.6990\n",
            "Final, Train Loss: 0.0476, Test Loss: 0.4258\n",
            "Final, Train Loss: 2.1889, Test Loss: 2.1555\n",
            "Final, Train Loss: 1.0889, Test Loss: 0.5313\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1779\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0964\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1362\n",
            "Final, Train Loss: 0.0006, Test Loss: 0.0769\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.3194\n",
            "Final, Train Loss: 0.1132, Test Loss: 0.4826\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.3511\n",
            "Final, Train Loss: 0.1975, Test Loss: 0.1361\n",
            "Final, Train Loss: 0.0444, Test Loss: 0.5672\n",
            "Final, Train Loss: 0.0002, Test Loss: 0.0817\n",
            "Final, Train Loss: 0.0937, Test Loss: 0.7164\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2495\n",
            "Final, Train Loss: 1.8274, Test Loss: 1.9169\n",
            "Final, Train Loss: 2.3033, Test Loss: 2.3022\n",
            "Final, Train Loss: 1.6200, Test Loss: 2.3105\n",
            "Final, Train Loss: 2.3062, Test Loss: 2.3021\n",
            "Final, Train Loss: 2.3056, Test Loss: 2.3013\n",
            "Final, Train Loss: 2.1334, Test Loss: 2.0490\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2111\n",
            "Final, Train Loss: 0.0831, Test Loss: 0.4959\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1397\n",
            "Final, Train Loss: 0.1140, Test Loss: 0.5212\n",
            "Final, Train Loss: 1.3208, Test Loss: 1.7032\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.3030\n",
            "Final, Train Loss: 0.0327, Test Loss: 0.7529\n",
            "Final, Train Loss: 0.0501, Test Loss: 0.7031\n",
            "Final, Train Loss: 2.0190, Test Loss: 2.2513\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0993\n",
            "Final, Train Loss: 0.0624, Test Loss: 0.8515\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1384\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2738\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1229\n",
            "Final, Train Loss: 0.8551, Test Loss: 0.6336\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1204\n",
            "Final, Train Loss: 0.0344, Test Loss: 0.5174\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0985\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.2784\n",
            "Final, Train Loss: 0.1176, Test Loss: 0.7488\n",
            "Final, Train Loss: 2.3047, Test Loss: 2.3015\n",
            "Final, Train Loss: 2.1130, Test Loss: 2.0974\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1038\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1082\n",
            "Final, Train Loss: 2.2091, Test Loss: 1.9663\n",
            "Final, Train Loss: 0.0274, Test Loss: 0.2662\n",
            "Final, Train Loss: 0.0479, Test Loss: 0.4492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f69a983b-b65e-4185-b56c-b6fabd784c4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f69a983b-b65e-4185-b56c-b6fabd784c4e",
        "outputId": "9b3ae363-9684-4ebd-d4e2-df7866b987d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beta 1: 0.21\n",
            "Beta 2: 0.69\n",
            "Epsilon: 4.27e-03\n",
            "Learning rate: 0.000427\n",
            "Train error: 0.000572\n",
            "Test error: 0.0769\n"
          ]
        }
      ],
      "source": [
        "# Print best parameters\n",
        "\n",
        "print('Beta 1: %.2f' % lowest_test_error[0])\n",
        "print('Beta 2: %.2f' % lowest_test_error[1])\n",
        "print('Epsilon: %.2e' % lowest_test_error[2])\n",
        "print('Learning rate: %.6f' % lowest_test_error[3])\n",
        "print('Train error: %.6f' % lowest_test_error[4])\n",
        "print('Test error: %.4f' % lowest_test_error[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5b6adf-dac0-4e2e-a3f4-27186b406140",
      "metadata": {
        "tags": [],
        "id": "3c5b6adf-dac0-4e2e-a3f4-27186b406140"
      },
      "source": [
        "##### 4.2.3. Estimating trial outcomes via bootstrap\n",
        "At this stage we want to estimate means and uncertainties of our tuning protocol"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db030739-278f-4fe8-9932-b4af662923ff",
      "metadata": {
        "id": "db030739-278f-4fe8-9932-b4af662923ff"
      },
      "source": [
        "###### Run N trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad08fad-3e95-4d86-8d4d-38c2f43d6aa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ad08fad-3e95-4d86-8d4d-38c2f43d6aa3",
        "outputId": "4283a241-342e-42b3-d705-f89ea8ad8ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final, Train Loss: 0.0001, Test Loss: 0.3323\n",
            "Final, Train Loss: 1.8190, Test Loss: 1.9930\n",
            "Final, Train Loss: 2.3077, Test Loss: 2.3014\n",
            "Final, Train Loss: 1.6570, Test Loss: 2.6781\n",
            "Final, Train Loss: 2.3139, Test Loss: 2.2030\n",
            "Final, Train Loss: 1.9307, Test Loss: 2.3562\n",
            "Final, Train Loss: 2.3055, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0277, Test Loss: 0.6947\n",
            "Final, Train Loss: 2.3085, Test Loss: 2.3019\n",
            "Final, Train Loss: 1.5564, Test Loss: 0.9790\n",
            "Final, Train Loss: 0.0688, Test Loss: 0.5211\n",
            "Final, Train Loss: 2.1636, Test Loss: 2.0797\n",
            "Final, Train Loss: 1.1641, Test Loss: 1.3472\n",
            "Final, Train Loss: 1.3095, Test Loss: 0.7236\n",
            "Final, Train Loss: 0.0616, Test Loss: 0.5079\n",
            "Final, Train Loss: 2.0921, Test Loss: 1.6448\n",
            "Final, Train Loss: 2.3069, Test Loss: 2.3022\n",
            "Final, Train Loss: 2.0068, Test Loss: 2.3230\n",
            "Final, Train Loss: 2.3067, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.0792\n",
            "Final, Train Loss: 0.0012, Test Loss: 0.0760\n",
            "Final, Train Loss: 0.0294, Test Loss: 0.2719\n",
            "Final, Train Loss: 0.0582, Test Loss: 0.3365\n",
            "Final, Train Loss: 1.1182, Test Loss: 1.3806\n",
            "Final, Train Loss: 2.3069, Test Loss: 2.3020\n",
            "Final, Train Loss: 0.3465, Test Loss: 0.7455\n",
            "Final, Train Loss: 2.3064, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.3457, Test Loss: 0.6752\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.5160\n",
            "Final, Train Loss: 0.3972, Test Loss: 0.6288\n",
            "Final, Train Loss: 2.3082, Test Loss: 2.3027\n",
            "Final, Train Loss: 0.3386, Test Loss: 0.7945\n",
            "Final, Train Loss: 2.3065, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0622, Test Loss: 0.7603\n",
            "Final, Train Loss: 2.3095, Test Loss: 2.3023\n",
            "Final, Train Loss: 0.0890, Test Loss: 0.6952\n",
            "Final, Train Loss: 2.3063, Test Loss: 2.3014\n",
            "Final, Train Loss: 2.3046, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0842\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1960\n",
            "Final, Train Loss: 0.0716, Test Loss: 0.5401\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1763\n",
            "Final, Train Loss: 1.8487, Test Loss: 1.7735\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1010\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1202\n",
            "Final, Train Loss: 0.0442, Test Loss: 0.6209\n",
            "Final, Train Loss: 0.0011, Test Loss: 0.5853\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0903\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0895\n",
            "Final, Train Loss: 0.4413, Test Loss: 0.5144\n",
            "Final, Train Loss: 1.3403, Test Loss: 1.0217\n",
            "Final, Train Loss: 2.3057, Test Loss: 2.3022\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3030\n",
            "Final, Train Loss: 0.0264, Test Loss: 0.4658\n",
            "Final, Train Loss: 2.3007, Test Loss: 2.3039\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0909\n",
            "Final, Train Loss: 2.3085, Test Loss: 2.3019\n",
            "Final, Train Loss: 0.0017, Test Loss: 0.4732\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1383\n",
            "Final, Train Loss: 2.3073, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.4298\n",
            "Final, Train Loss: 0.0296, Test Loss: 0.7828\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1224\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1376\n",
            "Final, Train Loss: 0.0893, Test Loss: 0.3909\n",
            "Final, Train Loss: 0.0005, Test Loss: 0.0798\n",
            "Final, Train Loss: 0.0733, Test Loss: 0.5107\n",
            "Final, Train Loss: 2.3058, Test Loss: 2.3018\n",
            "Final, Train Loss: 0.0572, Test Loss: 0.6542\n",
            "Final, Train Loss: 0.7034, Test Loss: 0.9884\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3004\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0921\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2087\n",
            "Final, Train Loss: 0.0296, Test Loss: 0.6244\n",
            "Final, Train Loss: 2.3812, Test Loss: 1.3389\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1170\n",
            "Final, Train Loss: 0.0667, Test Loss: 0.7081\n",
            "Final, Train Loss: 0.0224, Test Loss: 0.4350\n",
            "Final, Train Loss: 0.0456, Test Loss: 0.4815\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2157\n",
            "Final, Train Loss: 1.8498, Test Loss: 1.8797\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1468\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0821\n",
            "Final, Train Loss: 0.1970, Test Loss: 0.5915\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1264\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1019\n",
            "Final, Train Loss: 0.0083, Test Loss: 0.2613\n",
            "Final, Train Loss: 2.1776, Test Loss: 2.1056\n",
            "Final, Train Loss: 0.0718, Test Loss: 0.5389\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0863\n",
            "Final, Train Loss: 2.3066, Test Loss: 2.3014\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1763\n",
            "Final, Train Loss: 0.0007, Test Loss: 0.3708\n",
            "Final, Train Loss: 0.1511, Test Loss: 0.5912\n",
            "Final, Train Loss: 2.3061, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.5070\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0779\n",
            "Final, Train Loss: 0.0219, Test Loss: 0.5972\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2407\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1919\n",
            "Final, Train Loss: 2.1987, Test Loss: 3.6654\n",
            "Final, Train Loss: 0.0482, Test Loss: 0.5809\n",
            "Final, Train Loss: 0.0637, Test Loss: 0.6450\n",
            "Final, Train Loss: 0.1016, Test Loss: 0.4565\n",
            "Final, Train Loss: 2.3182, Test Loss: 2.2277\n",
            "Final, Train Loss: 0.1255, Test Loss: 0.9728\n",
            "Final, Train Loss: 0.7614, Test Loss: 2.0057\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1207\n",
            "Final, Train Loss: 3.2574, Test Loss: 1.5328\n",
            "Final, Train Loss: 2.2692, Test Loss: 2.1618\n",
            "Final, Train Loss: 0.0332, Test Loss: 0.5677\n",
            "Final, Train Loss: 1.5492, Test Loss: 1.6509\n",
            "Final, Train Loss: 2.3072, Test Loss: 2.3016\n",
            "Final, Train Loss: 0.2438, Test Loss: 0.6538\n",
            "Final, Train Loss: 0.0590, Test Loss: 0.7003\n",
            "Final, Train Loss: 0.6451, Test Loss: 2.0763\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2545\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1029\n",
            "Final, Train Loss: 1.9813, Test Loss: 2.5686\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1612\n",
            "Final, Train Loss: 0.0784, Test Loss: 0.7375\n",
            "Final, Train Loss: 0.1009, Test Loss: 0.8878\n",
            "Final, Train Loss: 2.0958, Test Loss: 8.3748\n",
            "Final, Train Loss: 2.3040, Test Loss: 2.3011\n",
            "Final, Train Loss: 2.3096, Test Loss: 2.3026\n",
            "Final, Train Loss: 0.0455, Test Loss: 0.4822\n",
            "Final, Train Loss: 0.1259, Test Loss: 0.7752\n",
            "Final, Train Loss: 0.0344, Test Loss: 0.7780\n",
            "Final, Train Loss: 2.2376, Test Loss: 1.9129\n",
            "Final, Train Loss: 1.7767, Test Loss: 1.8556\n",
            "Final, Train Loss: 0.0024, Test Loss: 0.2691\n",
            "Final, Train Loss: 0.0610, Test Loss: 0.7122\n",
            "Final, Train Loss: 0.9780, Test Loss: 1.5027\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.0812\n",
            "Final, Train Loss: 2.3053, Test Loss: 2.3017\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.3424\n",
            "Final, Train Loss: 0.0695, Test Loss: 1.2145\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.0903\n",
            "Final, Train Loss: 0.0852, Test Loss: 0.6646\n",
            "Final, Train Loss: 0.0539, Test Loss: 0.6897\n",
            "Final, Train Loss: 0.0519, Test Loss: 0.8233\n",
            "Final, Train Loss: 0.0177, Test Loss: 0.5054\n",
            "Final, Train Loss: 2.3066, Test Loss: 2.3014\n",
            "Final, Train Loss: 2.1793, Test Loss: 1.9572\n",
            "Final, Train Loss: 0.0229, Test Loss: 0.2419\n",
            "Final, Train Loss: 1.9150, Test Loss: 1.8055\n",
            "Final, Train Loss: 0.0448, Test Loss: 0.5584\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1684\n",
            "Final, Train Loss: 0.1751, Test Loss: 0.7452\n",
            "Final, Train Loss: 2.2340, Test Loss: 2.5444\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1668\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.0947\n",
            "Final, Train Loss: 0.0048, Test Loss: 0.3613\n",
            "Final, Train Loss: 0.0342, Test Loss: 0.4002\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.4060\n",
            "Final, Train Loss: 2.3048, Test Loss: 2.3016\n",
            "Final, Train Loss: 2.3047, Test Loss: 2.3011\n",
            "Final, Train Loss: 0.0284, Test Loss: 0.5373\n",
            "Final, Train Loss: 2.3052, Test Loss: 2.3036\n",
            "Final, Train Loss: 0.0378, Test Loss: 0.5642\n",
            "Final, Train Loss: 2.1666, Test Loss: 1.9663\n",
            "Final, Train Loss: 0.0586, Test Loss: 0.5746\n",
            "Final, Train Loss: 2.3089, Test Loss: 2.3020\n",
            "Final, Train Loss: 1.2322, Test Loss: 1.0717\n",
            "Final, Train Loss: 0.3482, Test Loss: 0.9416\n",
            "Final, Train Loss: 0.0469, Test Loss: 1.3146\n",
            "Final, Train Loss: 1.9858, Test Loss: 2.3714\n",
            "Final, Train Loss: 0.3354, Test Loss: 0.9037\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1177\n",
            "Final, Train Loss: 2.5075, Test Loss: 1.1668\n",
            "Final, Train Loss: 7.3731, Test Loss: 2.1247\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.0800\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1480\n",
            "Final, Train Loss: 3.9607, Test Loss: 1.6224\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1033\n",
            "Final, Train Loss: 0.0478, Test Loss: 0.8397\n",
            "Final, Train Loss: 0.0401, Test Loss: 0.4872\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1732\n",
            "Final, Train Loss: 2.1006, Test Loss: 1.9854\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1311\n",
            "Final, Train Loss: 0.0647, Test Loss: 0.4739\n",
            "Final, Train Loss: 0.0003, Test Loss: 0.0769\n",
            "Final, Train Loss: 1.6127, Test Loss: 2.2761\n",
            "Final, Train Loss: 2.3036, Test Loss: 2.3027\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1007\n",
            "Final, Train Loss: 0.0698, Test Loss: 0.5877\n",
            "Final, Train Loss: 2.3073, Test Loss: 2.3015\n",
            "Final, Train Loss: 0.0297, Test Loss: 0.4074\n",
            "Final, Train Loss: 2.3051, Test Loss: 2.3020\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1179\n",
            "Final, Train Loss: 0.0743, Test Loss: 0.4198\n",
            "Final, Train Loss: 0.0001, Test Loss: 0.1007\n",
            "Final, Train Loss: 0.0093, Test Loss: 0.4644\n",
            "Final, Train Loss: 2.0617, Test Loss: 1.9476\n",
            "Final, Train Loss: 2.3037, Test Loss: 2.3024\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.1819\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.3140\n",
            "Final, Train Loss: 0.0744, Test Loss: 0.6812\n",
            "Final, Train Loss: 0.0000, Test Loss: 0.2877\n"
          ]
        }
      ],
      "source": [
        "# We first run and store N trials\n",
        "N=200\n",
        "N_trials = []\n",
        "\n",
        "\n",
        "for _ in range(N):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model = MLP()\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Store parameters, train and test error\n",
        "    N_trials.append([beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"N_trials_adam_mlp.pth\", \"wb\") as fp:\n",
        "  pickle.dump(N_trials, fp)"
      ],
      "metadata": {
        "id": "031_uIdTsEhC"
      },
      "id": "031_uIdTsEhC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f72811ee-a1d8-408c-b3a5-e23b9b055bec",
      "metadata": {
        "id": "f72811ee-a1d8-408c-b3a5-e23b9b055bec"
      },
      "source": [
        "###### Perform bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059f03aa-7234-4943-a858-7e68e84e33d2",
      "metadata": {
        "id": "059f03aa-7234-4943-a858-7e68e84e33d2"
      },
      "outputs": [],
      "source": [
        "means_train = []\n",
        "means_test  = []\n",
        "# N_trials = N_trials.to('cpu')\n",
        "# Do the following 100 times :\n",
        "for _ in range(100):\n",
        "    # Resample N samples from the N-trials with replacement\n",
        "    N_sampled_indices = np.random.choice(list(range(len(N_trials))), N) # choose random indices in the list of N trials\n",
        "    \n",
        "    # Recover the lists associated to the indices and keep only intersting information, i.e. test and train errors\n",
        "    N_sampled_train_error = np.array([N_trials[i][4].cpu().detach().numpy() for i in N_sampled_indices])\n",
        "    N_sampled_test_error = np.array([N_trials[i][5].cpu().detach().numpy() for i in N_sampled_indices])\n",
        "    \n",
        "    # Compute statistic on the first K trials of the resampled dataset\n",
        "    means_train.append(N_sampled_train_error[:K].mean())\n",
        "    means_test.append(N_sampled_test_error[:K].mean())\n",
        "    \n",
        "# 5th percentile, 95 percentile of bootrap distribution\n",
        "fifth_percentile_train = np.percentile(means_train, 5)\n",
        "fifth_percentile_test = np.percentile(means_test, 5)\n",
        "\n",
        "ninety_fifth_percentile_train = np.percentile(means_train, 95)\n",
        "ninety_fifth_percentile_test = np.percentile(means_test, 95)\n",
        "\n",
        "# For plotting purposes only\n",
        "mean_all_train = np.array(means_train).mean()\n",
        "mean_all_test = np.array(means_test).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dfb7161-d1a2-4c08-be7e-32ff34fe3be0",
      "metadata": {
        "id": "9dfb7161-d1a2-4c08-be7e-32ff34fe3be0"
      },
      "source": [
        "###### Generate plots for mean error bars for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d78e184-b512-4f56-83e8-bd0bc6b927df",
      "metadata": {
        "id": "9d78e184-b512-4f56-83e8-bd0bc6b927df",
        "outputId": "a8ceb51b-0ec5-4100-fbea-ac03a741e5df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train plot, each index in x will be a different optimizer and y its values\n",
        "plt.style.use('seaborn-white')\n",
        "plt.scatter(x=[0], y=[mean_all_train])\n",
        "plt.errorbar(x=[0], y=[mean_all_train], yerr=[[fifth_percentile_train],[ninety_fifth_percentile_train]], ecolor='red', color='black')\n",
        "plt.ylabel('Train Loss')\n",
        "plt.savefig('Train_adam_mlp.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b96b7ad-c939-497b-8ab2-a31364ded430",
      "metadata": {
        "id": "4b96b7ad-c939-497b-8ab2-a31364ded430"
      },
      "source": [
        "###### Generate plots for mean error bars for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311df6f3-7d51-4b53-bc91-dae7c5ddff15",
      "metadata": {
        "id": "311df6f3-7d51-4b53-bc91-dae7c5ddff15",
        "outputId": "3e5847b2-cdab-404b-c47e-e0d050248f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train plot, each index in x will be a different optimizer and y its values\n",
        "plt.style.use('seaborn-white')\n",
        "plt.scatter(x=[0], y=[mean_all_test])\n",
        "plt.errorbar(x=[0], y=[mean_all_test], yerr=[[fifth_percentile_test],[ninety_fifth_percentile_test]], ecolor='red', color='black')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.savefig('Test_adam_mlp.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Tuning Adam for the CNN on MNIST"
      ],
      "metadata": {
        "id": "DBRfSh49xasS"
      },
      "id": "DBRfSh49xasS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuStHJoqzl5Y"
      },
      "source": [
        "##### Set up model for training"
      ],
      "id": "VuStHJoqzl5Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfnUAu5xzl5Z"
      },
      "outputs": [],
      "source": [
        "# Model fixed parameters\n",
        "model = CNN()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device) # good loss function for classification tasks\n",
        "num_epoch = 50\n",
        "size_minibatch = 128\n",
        "\n",
        "x_train = features_train\n",
        "y_train = labels_train\n",
        "x_test = features_test\n",
        "y_test = labels_test"
      ],
      "id": "FfnUAu5xzl5Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "t1eNEdgdzl5a"
      },
      "source": [
        "##### Tune to find best parameter\n",
        "We perform trials until we have K of them, then we pick the best based on our statistic of interest"
      ],
      "id": "t1eNEdgdzl5a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.1. Initial search for best hyperparameters for **Adam** optimizer on **CNN**. K= 100. \n",
        "Interrupted because of nan loss."
      ],
      "metadata": {
        "id": "MSu9ppsnzl5b"
      },
      "id": "MSu9ppsnzl5b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Set up parameters and search space for the initial trial"
      ],
      "metadata": {
        "id": "oyss_Dmczl5b"
      },
      "id": "oyss_Dmczl5b"
    },
    {
      "cell_type": "code",
      "source": [
        "N = 200\n",
        "K = 100 # Number of trials being kept for the statistic\n",
        "\n",
        "# Initial search spaces for parameters\n",
        "alpha_0 = np.linspace(10**(-2), 10**(4), N)\n",
        "beta_1 = np.linspace(10**(-3), 1, N)\n",
        "beta_2 = np.linspace(10**(-4), 1, N)\n",
        "eps = np.linspace(10**(-10), 10**(10), N)\n",
        "\n",
        "# TODO: tune number of decay steps between 0.5 and 1 times the number of training steps\n",
        "# TODO : tune learning rate decay factor within 10**-3, 10**-2, 10**-1"
      ],
      "metadata": {
        "id": "f6umyX4jzl5c"
      },
      "execution_count": null,
      "outputs": [],
      "id": "f6umyX4jzl5c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform search"
      ],
      "metadata": {
        "id": "4yWE5lNhzl5d"
      },
      "id": "4yWE5lNhzl5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMz4BHWrzl5d"
      },
      "outputs": [],
      "source": [
        "nb_hyperamaters_to_tune = 4\n",
        "nb_exported_statistics  = 2\n",
        "\n",
        "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
        "\n",
        "for _ in range(K):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model=CNN()\n",
        "    model=model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    \n",
        "    # Concatenate hyperparameters with results\n",
        "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
        "    \n",
        "    # Check wether we have the smallest test error and store parameters in case we find it\n",
        "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
        "        lowest_test_error = vector"
      ],
      "id": "aMz4BHWrzl5d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.2. Final search for best hyperparameters for **Adam** optimizer on **CNN**. K = 50"
      ],
      "metadata": {
        "id": "G97NRkQ70yBX"
      },
      "id": "G97NRkQ70yBX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Set up parameters and search space for the final trial"
      ],
      "metadata": {
        "id": "BcWL5tIe0yBY"
      },
      "id": "BcWL5tIe0yBY"
    },
    {
      "cell_type": "code",
      "source": [
        "N = 200\n",
        "K = 50 # Number of trials being kept for the statistic\n",
        "\n",
        "# Final search spaces for parameters\n",
        "alpha_0 = np.linspace(10**(-1), 10, N)\n",
        "beta_1 = np.linspace(10**(-3), 1, N)\n",
        "beta_2 = np.linspace(10**(-4), 1, N)\n",
        "eps = np.linspace(10**(-6), 10**(-2), N)"
      ],
      "metadata": {
        "id": "2LHRX7mT0yBY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2LHRX7mT0yBY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform search"
      ],
      "metadata": {
        "id": "VSruqZFS0yBZ"
      },
      "id": "VSruqZFS0yBZ"
    },
    {
      "cell_type": "code",
      "source": [
        "nb_hyperamaters_to_tune = 4\n",
        "nb_exported_statistics  = 2\n",
        "\n",
        "lowest_test_error = [sys.maxsize] * (nb_hyperamaters_to_tune + nb_exported_statistics)\n",
        "\n",
        "\n",
        "for _ in range(K):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model=CNN()\n",
        "    model=model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Concatenate hyperparameters with results\n",
        "    vector = [beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error]\n",
        "    \n",
        "    # Check wether we have the smallest test error and store parameters in case we find it\n",
        "    if test_error < lowest_test_error[len(lowest_test_error) - 1]:\n",
        "        lowest_test_error = vector"
      ],
      "metadata": {
        "id": "Mormnp9dw7Z9"
      },
      "id": "Mormnp9dw7Z9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print best parameters\n",
        "\n",
        "print('Beta 1: %.2f' % lowest_test_error[0])\n",
        "print('Beta 2: %.2f' % lowest_test_error[1])\n",
        "print('Epsilon: %.2e' % lowest_test_error[2])\n",
        "print('Learning rate: %.2f' % lowest_test_error[3])\n",
        "print('Train error: %.6f' % lowest_test_error[4])\n",
        "print('Test error: %.4f' % lowest_test_error[5])"
      ],
      "metadata": {
        "id": "52xLPeV8w3ZX"
      },
      "id": "52xLPeV8w3ZX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "mbJtvIFv0yBd"
      },
      "source": [
        "##### 4.3.3. Estimating trial outcomes via bootstrap\n",
        "At this stage we want to estimate means and uncertainties of our tuning protocol"
      ],
      "id": "mbJtvIFv0yBd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IohNC8cu0yBd"
      },
      "source": [
        "###### Run N trials"
      ],
      "id": "IohNC8cu0yBd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJjhEOxk0yBf"
      },
      "source": [
        "###### Perform bootstrapping"
      ],
      "id": "QJjhEOxk0yBf"
    },
    {
      "cell_type": "code",
      "source": [
        "# We first run and store N trials\n",
        "N=200\n",
        "N_trials = []\n",
        "\n",
        "for _ in range(N):\n",
        "    # Pick random values from the intervals given for the different parameters\n",
        "    alpha_0_pick  = float(np.random.choice(alpha_0, 1)) # np.random.choice samples uniformely with replacement\n",
        "    beta_1_pick   = float(-np.random.choice(beta_1, 1) + 1)\n",
        "    beta_2_pick   = float(-np.random.choice(beta_2, 1) + 1)\n",
        "    eps_pick      = float(np.random.choice(eps, 1))\n",
        "    learning_rate = alpha_0_pick * eps_pick\n",
        "    \n",
        "    # Build optimizer from parameters\n",
        "    model = CNN()\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1_pick, beta_2_pick), eps=eps_pick)\n",
        "    \n",
        "    # Run\n",
        "    train_error, test_error = run_nn(x_train,y_train, x_test, y_test, model, optimizer, criterion, num_epoch, size_minibatch)\n",
        "    \n",
        "    # Store parameters, train and test error\n",
        "    N_trials.append([beta_1_pick, beta_2_pick, eps_pick, learning_rate, train_error, test_error])"
      ],
      "metadata": {
        "id": "eDtMp9RuxGUJ"
      },
      "id": "eDtMp9RuxGUJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SqLoaXH0yBf"
      },
      "outputs": [],
      "source": [
        "means_train = []\n",
        "means_test  = []\n",
        "# Do the following 100 times :\n",
        "for _ in range(100):\n",
        "    # Resample N samples from the N-trials with replacement\n",
        "    N_sampled_indices = np.random.choice(list(range(len(N_trials))), N) # choose random indices in the list of N trials\n",
        "    \n",
        "    # Recover the lists associated to the indices and keep only intersting information, i.e. test and train errors\n",
        "    N_sampled_train_error = np.array([N_trials[i][4] for i in N_sampled_indices])\n",
        "    N_sampled_test_error = np.array([N_trials[i][5] for i in N_sampled_indices])\n",
        "    \n",
        "    # Compute statistic on the first K trials of the resampled dataset\n",
        "    means_train.append(N_sampled_train_error[:K].mean())\n",
        "    means_test.append(N_sampled_test_error[:K].mean())\n",
        "    \n",
        "# 5th percentile, 95 percentile of bootrap distribution\n",
        "fifth_percentile_train = np.percentile(means_train, 5)\n",
        "fifth_percentile_test = np.percentile(means_test, 5)\n",
        "\n",
        "ninety_fifth_percentile_train = np.percentile(means_train, 95)\n",
        "ninety_fifth_percentile_test = np.percentile(means_test, 95)\n",
        "\n",
        "# For plotting purposes only\n",
        "mean_all_train = np.array(means_train).mean()\n",
        "mean_all_test = np.array(means_test).mean()"
      ],
      "id": "-SqLoaXH0yBf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSP6WEa60yBg"
      },
      "source": [
        "###### Generate plots for mean error bars for training"
      ],
      "id": "cSP6WEa60yBg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train plot, each index in x will be a different optimizer and y its values\n",
        "plt.scatter(x=[0], y=[mean_all_train])\n",
        "plt.errorbar(x=[0], y=[mean_all_train], yerr=[[fifth_percentile_train],[ninety_fifth_percentile_train]], ecolor='red', color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QHofMNmExKGN"
      },
      "id": "QHofMNmExKGN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p70_yxIt0yBh"
      },
      "source": [
        "###### Generate plots for mean error bars for testing"
      ],
      "id": "p70_yxIt0yBh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train plot, each index in x will be a different optimizer and y its values\n",
        "plt.scatter(x=[0], y=[mean_all_test])\n",
        "plt.errorbar(x=[0], y=[mean_all_test], yerr=[[fifth_percentile_test],[ninety_fifth_percentile_test]], ecolor='red', color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cUgUgtt4xMKN"
      },
      "id": "cUgUgtt4xMKN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a31065c2cb285fc89af45a6e0e00b20f2f53e470cc5659f0bf6d768d5279787d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 ('knvenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "Comparing Optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}